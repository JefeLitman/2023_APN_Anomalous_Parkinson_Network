{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "endangered-assets",
   "metadata": {},
   "source": [
    "# GANomaly Notebook Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embedded-billy",
   "metadata": {},
   "source": [
    "## Initial Configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protected-meter",
   "metadata": {},
   "source": [
    "### Libraries import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ignored-motorcycle",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import umap\n",
    "import cv2\n",
    "from IPython.display import display, clear_output\n",
    "from ipywidgets import interact, IntSlider\n",
    "\n",
    "sys.path.append(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bored-portuguese",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.metrics import shapiroWilks_test, dagostinoPearson_test, bartlett_test, levene_test, fOneWay_test\n",
    "from utils.savers import generate_qq_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eec12b3",
   "metadata": {},
   "source": [
    "### Experiment selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5551c490",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jefelitman/Saved_Models/Anomaly_parkinson/0010_Ganomaly_2D-256x256x64'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_id = \"0010\"\n",
    "root_path = \"/home/jefelitman/Saved_Models/Anomaly_parkinson/\"\n",
    "for i in sorted(os.listdir(\"/home/jefelitman/Saved_Models/Anomaly_parkinson/\")):\n",
    "    if experiment_id in i:\n",
    "        experiment_folder = os.path.join(root_path, i)\n",
    "experiment_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "labeled-audit",
   "metadata": {},
   "source": [
    "## Quantitative metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norwegian-decline",
   "metadata": {},
   "source": [
    "### Metrics loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "optimum-genetics",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(epoch          29999.000000\n",
       " gen_error        660.663879\n",
       " disc_error         7.817431\n",
       " accuracy           0.930556\n",
       " precision          0.000000\n",
       " recall                  NaN\n",
       " specificity        0.930556\n",
       " f1_score           0.000000\n",
       " auc                0.000000\n",
       " dtype: float64,\n",
       " epoch          0.000000\n",
       " gen_error      0.590776\n",
       " disc_error     2.942470\n",
       " accuracy       0.291667\n",
       " precision      0.000000\n",
       " recall              NaN\n",
       " specificity    0.291667\n",
       " f1_score       0.000000\n",
       " auc            0.000000\n",
       " dtype: float64,\n",
       " array([2.99990000e+04, 6.23744249e-01, 7.62461948e+00, 6.52777791e-01,\n",
       "        0.00000000e+00,            nan, 6.52777791e-01, 0.00000000e+00,\n",
       "        0.00000000e+00]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_metrics = pd.read_csv(os.path.join(experiment_folder, \"metrics/train.csv\"))\n",
    "data = train_metrics\n",
    "np.max(data, axis=0), np.min(data, axis=0), data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "confidential-guest",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(epoch          29999.000000\n",
       " accuracy           0.855769\n",
       " precision          1.000000\n",
       " recall             0.829545\n",
       " specificity        1.000000\n",
       " f1_score           0.906832\n",
       " auc                1.000000\n",
       " dtype: float64,\n",
       " epoch          0.000000\n",
       " accuracy       0.067308\n",
       " precision      0.111111\n",
       " recall         0.011364\n",
       " specificity    0.000000\n",
       " f1_score       0.020619\n",
       " auc            0.009943\n",
       " dtype: float64,\n",
       "        epoch  accuracy  precision    recall  specificity  f1_score       auc\n",
       " 29995  29995  0.605769        1.0  0.534091          1.0  0.696296  1.000000\n",
       " 29996  29996  0.644231        1.0  0.579545          1.0  0.733813  0.991832\n",
       " 29997  29997  0.567308        1.0  0.488636          1.0  0.656489  1.000000\n",
       " 29998  29998  0.653846        1.0  0.590909          1.0  0.742857  0.994318\n",
       " 29999  29999  0.625000        1.0  0.556818          1.0  0.715328  0.994318)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_metrics = pd.read_csv(os.path.join(experiment_folder, \"metrics/test.csv\"))\n",
    "data = test_metrics\n",
    "np.max(data, axis=0), np.min(data, axis=0), data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blessed-columbia",
   "metadata": {},
   "source": [
    "### Train Graphics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "completed-round",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(experiment_folder, \"outputs/graphics/quantitative/\")\n",
    "portions = 500\n",
    "rows = 12\n",
    "columns = 5\n",
    "for metric in [\"gen_error\", \"disc_error\", \"accuracy\", \"specificity\"]:\n",
    "    fig, axs = plt.subplots(rows, columns, figsize=(30, 25))\n",
    "    fig.suptitle('{} over Epochs'.format(metric))\n",
    "    for i in range(rows):\n",
    "        for j in range(columns):\n",
    "            axs[i, j].plot(train_metrics[metric][(i*columns+j)*portions:(i*columns+j+1)*portions])\n",
    "            filename = 'train_{}.png'.format(metric)\n",
    "    fig.savefig(path+filename)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latin-giant",
   "metadata": {},
   "source": [
    "### Test graphics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "least-melissa",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(experiment_folder, \"outputs/graphics/quantitative/\")\n",
    "portions = 500\n",
    "rows = 12\n",
    "columns = 5\n",
    "for metric in test_metrics.columns[1:]:\n",
    "    fig, axs = plt.subplots(rows, columns, figsize=(30, 25))\n",
    "    fig.suptitle('{} over Epochs'.format(metric))\n",
    "    for i in range(rows):\n",
    "        for j in range(columns):\n",
    "            axs[i, j].plot(test_metrics[metric][(i*columns+j)*portions:(i*columns+j+1)*portions])\n",
    "            filename = 'test_{}.png'.format(metric)\n",
    "    fig.savefig(path+filename)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "olympic-recipient",
   "metadata": {},
   "source": [
    "## Qualitative metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "french-handy",
   "metadata": {},
   "source": [
    "### Errors loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "configured-tyler",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.path.join(experiment_folder, \"outputs/errors/\")\n",
    "for t in [\"encoder\", \"contextual\", \"adversarial\"]:\n",
    "    for m in [\"train\", \"test\"]:\n",
    "        classes = [\"normal\"] if m == \"train\" else [\"normal\", \"abnormal\"]\n",
    "        for c in classes:\n",
    "            all_data = \"all_{}_{}\".format(t, c)\n",
    "            if all_data not in globals().keys():\n",
    "                globals()[all_data] = np.r_[[]]\n",
    "            \n",
    "            errors = np.load(os.path.join(base_path, t, m, c+\".npy\"))\n",
    "            globals()[\"{}_{}_{}\".format(m, t, c)] = errors\n",
    "            globals()[all_data] = np.concatenate([globals()[all_data], errors])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dynamic-remains",
   "metadata": {},
   "source": [
    "### Train means and stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "disciplinary-label",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder error (normal): 0.0005634856750273482 +- 0.00030529587669815783\n",
      "\n",
      "contextual error (normal): 0.01207544998679724 +- 0.0017062103510774049\n",
      "\n",
      "adversarial error (normal): 0.018182152274271682 +- 0.005344321682463277\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for t in [\"encoder\", \"contextual\", \"adversarial\"]:\n",
    "    for c in [\"normal\"]:\n",
    "        data = globals()[\"train_{}_{}\".format(t,c)]\n",
    "        print(\"{i} error ({j}): {m} +- {s}\".format(\n",
    "            i = t,\n",
    "            j = c,\n",
    "            m = np.mean(data),\n",
    "            s = np.std(data)\n",
    "        ))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opponent-status",
   "metadata": {},
   "source": [
    "### Train statistical tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "continuing-installation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "D'agostino-Pearson test for encoder (normal): 0.003501273764339694\n",
      "Shapiro-Wilks test for encoder (normal): 0.0003386996977496892\n",
      "Barlett test for encoder (normal): 1.4190204114689143e-228\n",
      "Levene test for encoder (normal): 3.285423233662101e-22\n",
      "F1 test for encoder (normal): 0.6244774456095635\n",
      "-------------------------------------------------\n",
      "\n",
      "-------------------------------------------------\n",
      "D'agostino-Pearson test for contextual (normal): 0.03813721381714772\n",
      "Shapiro-Wilks test for contextual (normal): 0.006934409029781818\n",
      "Barlett test for contextual (normal): 9.0721367117138e-175\n",
      "Levene test for contextual (normal): 8.309428763581047e-21\n",
      "F1 test for contextual (normal): 0.21540922536466797\n",
      "-------------------------------------------------\n",
      "\n",
      "-------------------------------------------------\n",
      "D'agostino-Pearson test for adversarial (normal): 0.007582750746299013\n",
      "Shapiro-Wilks test for adversarial (normal): 3.836040559690446e-05\n",
      "Barlett test for adversarial (normal): 2.3424965120964716e-144\n",
      "Levene test for adversarial (normal): 1.164069071762739e-24\n",
      "F1 test for adversarial (normal): 0.11630203641744169\n",
      "-------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for t in [\"encoder\", \"contextual\", \"adversarial\"]:\n",
    "    for c in [\"normal\"]:\n",
    "        print(\"-------------------------------------------------\")\n",
    "        data = globals()[\"train_{}_{}\".format(t,c)]\n",
    "        norm_dist = stats.norm.rvs(loc=0, scale=1, size=data.shape)\n",
    "        print(\"D'agostino-Pearson test for {i} ({j}): {m}\".format(\n",
    "            i = t,\n",
    "            j = c,\n",
    "            m = dagostinoPearson_test(data)\n",
    "        ))\n",
    "        print(\"Shapiro-Wilks test for {i} ({j}): {m}\".format(\n",
    "            i = t,\n",
    "            j = c,\n",
    "            m = shapiroWilks_test(data)\n",
    "        ))\n",
    "        print(\"Barlett test for {i} ({j}): {m}\".format(\n",
    "            i = t,\n",
    "            j = c,\n",
    "            m = bartlett_test(data, norm_dist)\n",
    "        ))\n",
    "        print(\"Levene test for {i} ({j}): {m}\".format(\n",
    "            i = t,\n",
    "            j = c,\n",
    "            m = levene_test(data, norm_dist)\n",
    "        ))\n",
    "        print(\"F1 test for {i} ({j}): {m}\".format(\n",
    "            i = t,\n",
    "            j = c,\n",
    "            m = fOneWay_test(data, norm_dist)\n",
    "        ))\n",
    "        print(\"-------------------------------------------------\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outside-indian",
   "metadata": {},
   "source": [
    "### Test means and stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "tired-jacob",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder error (normal): 0.0009448078462810372 +- 0.0004947018925274825\n",
      "encoder error (abnormal): 0.2359988685189323 +- 0.08662532752428148\n",
      "\n",
      "contextual error (normal): 0.01193352317204699 +- 0.001260851146065392\n",
      "contextual error (abnormal): 0.1929209144278006 +- 0.04577405023818982\n",
      "\n",
      "adversarial error (normal): 0.020554132701363415 +- 0.007025139944602236\n",
      "adversarial error (abnormal): 0.8737956065345894 +- 0.33418175548036283\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for t in [\"encoder\", \"contextual\", \"adversarial\"]:\n",
    "    for c in [\"normal\", \"abnormal\"]:\n",
    "        data = globals()[\"test_{}_{}\".format(t,c)]\n",
    "        print(\"{i} error ({j}): {m} +- {s}\".format(\n",
    "            i = t,\n",
    "            j = c,\n",
    "            m = np.mean(data),\n",
    "            s = np.std(data)\n",
    "        ))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electronic-remove",
   "metadata": {},
   "source": [
    "### Test statistical tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "funny-duncan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "D'agostino-Pearson test for encoder (normal): 0.6212754693632945\n",
      "Shapiro-Wilks test for encoder (normal): 0.5110328197479248\n",
      "Barlett test for encoder (normal): 9.907601504624316e-46\n",
      "Levene test for encoder (normal): 3.411273451665428e-10\n",
      "F1 test for encoder (normal): 0.23842314004259743\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "D'agostino-Pearson test for encoder (abnormal): 0.003626524972103315\n",
      "Shapiro-Wilks test for encoder (abnormal): 6.631091309827752e-06\n",
      "Barlett test for encoder (abnormal): 5.453108282552607e-69\n",
      "Levene test for encoder (abnormal): 2.8114704879457886e-21\n",
      "F1 test for encoder (abnormal): 0.041382934326941484\n",
      "-------------------------------------------------\n",
      "\n",
      "-------------------------------------------------\n",
      "D'agostino-Pearson test for contextual (normal): 0.2109994973979376\n",
      "Shapiro-Wilks test for contextual (normal): 0.15943466126918793\n",
      "Barlett test for contextual (normal): 2.207050991647907e-39\n",
      "Levene test for contextual (normal): 4.2112603616082597e-07\n",
      "F1 test for contextual (normal): 0.1734537090701074\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "D'agostino-Pearson test for contextual (abnormal): 3.511177674864577e-08\n",
      "Shapiro-Wilks test for contextual (abnormal): 3.9303337473484135e-08\n",
      "Barlett test for contextual (abnormal): 8.606653122532308e-93\n",
      "Levene test for contextual (abnormal): 1.8849284799988244e-28\n",
      "F1 test for contextual (abnormal): 0.7442235880307851\n",
      "-------------------------------------------------\n",
      "\n",
      "-------------------------------------------------\n",
      "D'agostino-Pearson test for adversarial (normal): 0.030557479738659448\n",
      "Shapiro-Wilks test for adversarial (normal): 0.02229372039437294\n",
      "Barlett test for adversarial (normal): 5.423195257670111e-27\n",
      "Levene test for adversarial (normal): 1.3913184622833352e-06\n",
      "F1 test for adversarial (normal): 0.9839183967848532\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "D'agostino-Pearson test for adversarial (abnormal): 0.03558804344762291\n",
      "Shapiro-Wilks test for adversarial (abnormal): 0.0012537251459434628\n",
      "Barlett test for adversarial (abnormal): 3.1273937831300468e-18\n",
      "Levene test for adversarial (abnormal): 7.198306585166549e-11\n",
      "F1 test for adversarial (abnormal): 2.366485147489225e-15\n",
      "-------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for t in [\"encoder\", \"contextual\", \"adversarial\"]:\n",
    "    for c in [\"normal\", \"abnormal\"]:\n",
    "        print(\"-------------------------------------------------\")\n",
    "        data = globals()[\"test_{}_{}\".format(t,c)]\n",
    "        norm_dist = stats.norm.rvs(loc=0, scale=1, size=data.shape)\n",
    "        print(\"D'agostino-Pearson test for {i} ({j}): {m}\".format(\n",
    "            i = t,\n",
    "            j = c,\n",
    "            m = dagostinoPearson_test(data)\n",
    "        ))\n",
    "        print(\"Shapiro-Wilks test for {i} ({j}): {m}\".format(\n",
    "            i = t,\n",
    "            j = c,\n",
    "            m = shapiroWilks_test(data)\n",
    "        ))\n",
    "        print(\"Barlett test for {i} ({j}): {m}\".format(\n",
    "            i = t,\n",
    "            j = c,\n",
    "            m = bartlett_test(data, norm_dist)\n",
    "        ))\n",
    "        print(\"Levene test for {i} ({j}): {m}\".format(\n",
    "            i = t,\n",
    "            j = c,\n",
    "            m = levene_test(data, norm_dist)\n",
    "        ))\n",
    "        print(\"F1 test for {i} ({j}): {m}\".format(\n",
    "            i = t,\n",
    "            j = c,\n",
    "            m = fOneWay_test(data, norm_dist)\n",
    "        ))\n",
    "        print(\"-------------------------------------------------\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b956fca",
   "metadata": {},
   "source": [
    "### All data means and stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "14458150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder error (normal): 0.0006328169788916553 +- 0.0003773409533503786\n",
      "encoder error (abnormal): 0.2359988685189323 +- 0.08662532752428148\n",
      "\n",
      "contextual error (normal): 0.012049645111388103 +- 0.0016352046499146817\n",
      "contextual error (abnormal): 0.1929209144278006 +- 0.04577405023818982\n",
      "\n",
      "adversarial error (normal): 0.018613421442833813 +- 0.005760112127483862\n",
      "adversarial error (abnormal): 0.8737956065345894 +- 0.33418175548036283\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for t in [\"encoder\", \"contextual\", \"adversarial\"]:\n",
    "    for c in [\"normal\", \"abnormal\"]:\n",
    "        data = globals()[\"all_{}_{}\".format(t,c)]\n",
    "        print(\"{i} error ({j}): {m} +- {s}\".format(\n",
    "            i = t,\n",
    "            j = c,\n",
    "            m = np.mean(data),\n",
    "            s = np.std(data)\n",
    "        ))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd8b130",
   "metadata": {},
   "source": [
    "### All data statistical tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6bbc99f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "D'agostino-Pearson test for encoder (normal): 3.240295364411539e-05\n",
      "Shapiro-Wilks test for encoder (normal): 6.965923148527509e-06\n",
      "Barlett test for encoder (normal): 3.700169203465865e-269\n",
      "Levene test for encoder (normal): 4.2271662075612e-24\n",
      "F1 test for encoder (normal): 0.7187395884344359\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "D'agostino-Pearson test for encoder (abnormal): 0.003626524972103315\n",
      "Shapiro-Wilks test for encoder (abnormal): 6.631091309827752e-06\n",
      "Barlett test for encoder (abnormal): 1.8259942449093752e-69\n",
      "Levene test for encoder (abnormal): 1.461788416269539e-21\n",
      "F1 test for encoder (abnormal): 0.06254602667137028\n",
      "-------------------------------------------------\n",
      "\n",
      "-------------------------------------------------\n",
      "D'agostino-Pearson test for contextual (normal): 0.021491927241864828\n",
      "Shapiro-Wilks test for contextual (normal): 0.0031588124111294746\n",
      "Barlett test for contextual (normal): 1.0755522569056578e-221\n",
      "Levene test for contextual (normal): 3.594044632081758e-26\n",
      "F1 test for contextual (normal): 0.3738762559567185\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "D'agostino-Pearson test for contextual (abnormal): 3.511177674864577e-08\n",
      "Shapiro-Wilks test for contextual (abnormal): 3.9303337473484135e-08\n",
      "Barlett test for contextual (abnormal): 8.235436313803481e-93\n",
      "Levene test for contextual (abnormal): 1.0003633908107773e-28\n",
      "F1 test for contextual (abnormal): 0.003991496343340332\n",
      "-------------------------------------------------\n",
      "\n",
      "-------------------------------------------------\n",
      "D'agostino-Pearson test for adversarial (normal): 0.006262975145267738\n",
      "Shapiro-Wilks test for adversarial (normal): 3.1522697554464685e-06\n",
      "Barlett test for adversarial (normal): 1.4414371213427555e-173\n",
      "Levene test for adversarial (normal): 3.231766862578449e-27\n",
      "F1 test for adversarial (normal): 0.6792575714759151\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "D'agostino-Pearson test for adversarial (abnormal): 0.03558804344762291\n",
      "Shapiro-Wilks test for adversarial (abnormal): 0.0012537251459434628\n",
      "Barlett test for adversarial (abnormal): 7.569707230815362e-22\n",
      "Levene test for adversarial (abnormal): 7.528218089324681e-15\n",
      "F1 test for adversarial (abnormal): 7.41337651373275e-13\n",
      "-------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for t in [\"encoder\", \"contextual\", \"adversarial\"]:\n",
    "    for c in [\"normal\", \"abnormal\"]:\n",
    "        print(\"-------------------------------------------------\")\n",
    "        data = globals()[\"all_{}_{}\".format(t,c)]\n",
    "        norm_dist = stats.norm.rvs(loc=0, scale=1, size=data.shape)\n",
    "        print(\"D'agostino-Pearson test for {i} ({j}): {m}\".format(\n",
    "            i = t,\n",
    "            j = c,\n",
    "            m = dagostinoPearson_test(data)\n",
    "        ))\n",
    "        print(\"Shapiro-Wilks test for {i} ({j}): {m}\".format(\n",
    "            i = t,\n",
    "            j = c,\n",
    "            m = shapiroWilks_test(data)\n",
    "        ))\n",
    "        print(\"Barlett test for {i} ({j}): {m}\".format(\n",
    "            i = t,\n",
    "            j = c,\n",
    "            m = bartlett_test(data, norm_dist)\n",
    "        ))\n",
    "        print(\"Levene test for {i} ({j}): {m}\".format(\n",
    "            i = t,\n",
    "            j = c,\n",
    "            m = levene_test(data, norm_dist)\n",
    "        ))\n",
    "        print(\"F1 test for {i} ({j}): {m}\".format(\n",
    "            i = t,\n",
    "            j = c,\n",
    "            m = fOneWay_test(data, norm_dist)\n",
    "        ))\n",
    "        print(\"-------------------------------------------------\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minus-processor",
   "metadata": {},
   "source": [
    "### Train graphics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "split-merchandise",
   "metadata": {},
   "source": [
    "#### QQ plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "motivated-plant",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join(experiment_folder, \"outputs/graphics/qualitative/\")\n",
    "for t in [\"encoder\", \"contextual\", \"adversarial\"]:\n",
    "    for c in [\"normal\"]:\n",
    "        generate_qq_plot(globals()[\"train_{}_{}\".format(t,c)], save_path, \"train_{}_{}\".format(t,c))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indoor-satin",
   "metadata": {},
   "source": [
    "### Test graphics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stone-panic",
   "metadata": {},
   "source": [
    "#### Box plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "desperate-lease",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "save_path = os.path.join(experiment_folder, \"outputs/graphics/qualitative/\")\n",
    "rows = 1\n",
    "columns = 3\n",
    "fig, axs = plt.subplots(rows, columns, figsize=(15, 10))\n",
    "for i, t in enumerate([\"encoder\", \"contextual\", \"adversarial\"]):\n",
    "    data = \"test_{}_\".format(t)\n",
    "    axs[i].boxplot([globals()[data+\"normal\"], globals()[data+\"abnormal\"]], labels=['Normal', 'Abnormal'])\n",
    "    axs[i].set_title(\"{} errors\".format(t))\n",
    "\n",
    "filename = 'test_boxplot.png'\n",
    "fig.savefig(save_path+filename)\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charged-multimedia",
   "metadata": {},
   "source": [
    "#### QQ plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "atlantic-assignment",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join(experiment_folder, \"outputs/graphics/qualitative/\")\n",
    "for t in [\"encoder\", \"contextual\", \"adversarial\"]:\n",
    "    for c in [\"normal\", \"abnormal\"]:\n",
    "        generate_qq_plot(globals()[\"test_{}_{}\".format(t,c)], save_path, \"test_{}_{}\".format(t,c))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d353b8",
   "metadata": {},
   "source": [
    "### All data graphics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55d922f",
   "metadata": {},
   "source": [
    "#### Box plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5ed03976",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join(experiment_folder, \"outputs/graphics/qualitative/\")\n",
    "rows = 1\n",
    "columns = 3\n",
    "fig, axs = plt.subplots(rows, columns, figsize=(15, 10))\n",
    "for i, t in enumerate([\"encoder\", \"contextual\", \"adversarial\"]):\n",
    "    data = \"all_{}_\".format(t)\n",
    "    axs[i].boxplot([globals()[data+\"normal\"], globals()[data+\"abnormal\"]], labels=['Normal', 'Abnormal'])\n",
    "    axs[i].set_title(\"{} errors\".format(t))\n",
    "\n",
    "filename = 'all_data_boxplot.png'\n",
    "fig.savefig(save_path+filename)\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291b27c9",
   "metadata": {},
   "source": [
    "#### QQ plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c433e281",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join(experiment_folder, \"outputs/graphics/qualitative/\")\n",
    "for t in [\"encoder\", \"contextual\", \"adversarial\"]:\n",
    "    for c in [\"normal\", \"abnormal\"]:\n",
    "        generate_qq_plot(globals()[\"all_{}_{}\".format(t,c)], save_path, \"all_data_{}_{}\".format(t,c))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "general-hudson",
   "metadata": {},
   "source": [
    "## Visual results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cultural-volume",
   "metadata": {},
   "source": [
    "### UMAP latent vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "yellow-alliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.path.join(experiment_folder, \"outputs/latent_vectors/\")\n",
    "for t in [\"in\", \"out\"]:\n",
    "    for m in [\"train\", \"test\"]:\n",
    "        classes = [\"normal\"] if m == \"train\" else [\"normal\", \"abnormal\"]\n",
    "        for c in classes:\n",
    "            all_data = \"all_{}_{}\".format(t, c)\n",
    "            if all_data not in globals().keys():\n",
    "                globals()[all_data] = []\n",
    "            \n",
    "            data = \"{}_{}_{}\".format(m, t, c)\n",
    "            globals()[data] = []\n",
    "            path = os.path.join(base_path, t, m, c)\n",
    "            for file in sorted(os.listdir(path)):\n",
    "                vector = np.load(os.path.join(path, file))\n",
    "                globals()[data].append(vector)\n",
    "                globals()[all_data].append(vector)\n",
    "            globals()[data] = np.r_[globals()[data]]\n",
    "for t in [\"in\", \"out\"]:\n",
    "    for c in [\"normal\", \"abnormal\"]:\n",
    "        all_data = \"all_{}_{}\".format(t, c)\n",
    "        globals()[all_data] = np.r_[globals()[all_data]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "apparent-sector",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join(experiment_folder, \"outputs/graphics/visuals/\")\n",
    "for t in [\"in\", \"out\"]:\n",
    "    for m in [\"train\", \"test\", \"all\"]:\n",
    "        classes = [\"normal\"] if m == \"train\" else [\"normal\", \"abnormal\"]\n",
    "        globals()[\"{}_mapped_{}\".format(m, t)] = umap.UMAP(random_state = 8128).fit_transform(np.concatenate(\n",
    "                [globals()[\"{}_{}_{}\".format(m, t, c)] for c in classes], axis=0\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        divisor = globals()[\"{}_{}_normal\".format(m, t)].shape[0]\n",
    "        plt.scatter(globals()[\"{}_mapped_{}\".format(m, t)][:divisor,0],\n",
    "            globals()[\"{}_mapped_{}\".format(m, t)][:divisor,1], color=\"blue\", s=5, label=\"Normal\"\n",
    "        )\n",
    "        if \"abnormal\" in classes:\n",
    "            plt.scatter(globals()[\"{}_mapped_{}\".format(m, t)][divisor:,0],\n",
    "                globals()[\"{}_mapped_{}\".format(m, t)][divisor:,1], color=\"red\", s=5, label=\"Abnormal\"\n",
    "            )\n",
    "        plt.legend()\n",
    "        plt.title(\"UMAP for {}put data in {}\".format(t, m))\n",
    "        filename = '{}_umap_{}put.png'.format(m, t)\n",
    "        plt.savefig(save_path+filename)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "younger-forth",
   "metadata": {},
   "source": [
    "### Video comparision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a3a18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_all_data(volumes):\n",
    "    frame_slider = IntSlider(value=32, min=1, max=volumes[0].shape[0], step=1)\n",
    "    volume_slider = IntSlider(min=1, max=len(volumes), step=1)\n",
    "\n",
    "    def update_frame_max(*args):\n",
    "        frame_slider.max = len(sorted(os.listdir(path)))\n",
    "    volume_slider.observe(update_frame_max, 'value')\n",
    "\n",
    "    interact(lambda volume, frame: plt.imshow(volumes[volume-1][frame-1,:,:,1], cmap=\"gray\"), \n",
    "             volume=volume_slider, frame=frame_slider\n",
    "    );\n",
    "show_all_data(classes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "narrow-classroom",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4114ef77e0394daab23c485af4bb78d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=32, description='frame', max=64, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dd9e18e2dd041889c7050509fd564b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=32, description='frame', max=64, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d8bf60ba52b425792dfc4d086d6f03f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=32, description='frame', max=64, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-d1164097f993>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0minteract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_fake\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwidget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0minteract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_substraction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwidget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"indice actual: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 851\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    852\u001b[0m         )\n\u001b[1;32m    853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    890\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 892\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "for i in range(0, 88):\n",
    "    video_index = i\n",
    "    mode = \"test\"\n",
    "    video_class = \"abnormal\"\n",
    "\n",
    "    base_path = os.path.join(experiment_folder, \"outputs/samples/\")\n",
    "    for t in [\"real\", \"fake\", \"substraction\"]:\n",
    "        path = os.path.join(base_path, t, mode, video_class)\n",
    "        video = sorted(os.listdir(path))[video_index]\n",
    "        data = \"video_{}\".format(t)\n",
    "        globals()[data] = []\n",
    "        for frame in sorted(os.listdir(os.path.join(path, video))):\n",
    "            globals()[data].append(cv2.cvtColor(\n",
    "                cv2.imread(\n",
    "                    os.path.join(path, video, frame)\n",
    "                ), cv2.COLOR_BGR2RGB\n",
    "            ))\n",
    "        globals()[data] = np.r_[globals()[data]]  \n",
    "\n",
    "    assert video_real.shape[0] == video_fake.shape[0] == video_substraction.shape[0]\n",
    "\n",
    "    widget = IntSlider(value=32, min=1, max=video_real.shape[0], step=1)\n",
    "\n",
    "    interact(lambda frame: plt.imshow(video_real[frame-1]), frame=widget)\n",
    "    interact(lambda frame: plt.imshow(video_fake[frame-1]), frame=widget)\n",
    "    interact(lambda frame: plt.imshow(video_substraction[frame-1]), frame=widget)\n",
    "    input(\"indice actual: {}\".format(i))\n",
    "    clear_output()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
