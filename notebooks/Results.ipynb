{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "endangered-assets",
   "metadata": {},
   "source": [
    "# Notebook for Individual Experiment Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protected-meter",
   "metadata": {},
   "source": [
    "### Libraries import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ignored-motorcycle",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bored-portuguese",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.metrics import brownForsythe_test, levene_test, bartlett_test\n",
    "from utils.metrics import accuracy, precision, recall, specificity, f1_score\n",
    "from utils.metrics import mannWhitney_test, kruskalWallis_test, kolmogorovSmirnov_test\n",
    "from utils.metrics import dagostinoPearson_test, andersonDarling_test, shapiroWilks_test, chiSquare_test, fOneWay_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f7c736",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_curve(y_true, y_pred, num_thresholds=200):\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    thresholds = np.linspace(np.min(y_pred), np.max(y_pred), num_thresholds)\n",
    "    for t in thresholds:\n",
    "        tp = np.count_nonzero(np.logical_and(y_true, (y_pred > t)))\n",
    "        fp = np.count_nonzero(np.logical_and(np.logical_not(y_true), (y_pred > t)))\n",
    "        fn = np.count_nonzero(np.logical_and(y_true, (y_pred <= t)))\n",
    "        if tp+fp == 0:\n",
    "            precisions.append(0)\n",
    "        else:\n",
    "            precisions.append(precision(tp, fp))\n",
    "        if tp + fn == 0:\n",
    "            recalls.append(0)\n",
    "        else:\n",
    "            recalls.append(recall(tp, fn))\n",
    "    return np.r_[precisions], np.r_[recalls], thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eec12b3",
   "metadata": {},
   "source": [
    "### Experiment selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5551c490",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 8128\n",
    "experiment_id = \"0006\"\n",
    "root_path = \"../results/Ganomaly_3D/\"\n",
    "for i in sorted(os.listdir(root_path)):\n",
    "    if experiment_id in i:\n",
    "        experiment_folder = os.path.join(root_path, i)\n",
    "experiment_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "labeled-audit",
   "metadata": {},
   "source": [
    "### Quantitative metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "french-handy",
   "metadata": {},
   "source": [
    "##### Errors loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "configured-tyler",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.path.join(experiment_folder, \"outputs/errors/\")\n",
    "for t in [\"encoder\", \"contextual\", \"adversarial\"]:\n",
    "    for c in [\"normal\", \"abnormal\"]:\n",
    "        globals()[\"all_{}_{}\".format(t, c)] = np.r_[[]]\n",
    "\n",
    "for t in [\"encoder\", \"contextual\", \"adversarial\"]:\n",
    "    for m in [\"train\", \"val\", \"test\"]:\n",
    "        if m == \"train\":\n",
    "            if os.path.isfile(os.path.join(base_path, t, m, \"normal.npy\")):\n",
    "                classes = [\"normal\"] \n",
    "            else:\n",
    "                classes = [\"abnormal\"]\n",
    "        else:\n",
    "            classes = [\"normal\", \"abnormal\"]\n",
    "\n",
    "        for c in classes:\n",
    "            all_data = \"all_{}_{}\".format(t, c)\n",
    "            errors = np.load(os.path.join(base_path, t, m, c + \".npy\"))\n",
    "            globals()[\"{}_{}_{}\".format(m, t, c)] = errors\n",
    "            globals()[all_data] = np.concatenate([globals()[all_data], errors])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d496d4f",
   "metadata": {},
   "source": [
    "##### Errors by patients loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94e8d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.path.join(experiment_folder, \"outputs/latent_vectors/input_generator\")\n",
    "for t in [\"encoder\", \"contextual\", \"adversarial\"]:\n",
    "    for c in [\"normal\", \"abnormal\"]:\n",
    "        globals()[\"all_{}_{}_patients\".format(t, c)] = {}\n",
    "\n",
    "for t in [\"encoder\", \"contextual\", \"adversarial\"]:\n",
    "    for m in [\"train\", \"val\", \"test\"]:\n",
    "        if m == \"train\":\n",
    "            if \"train_encoder_normal\" in globals().keys():\n",
    "                classes = [\"normal\"] \n",
    "            else:\n",
    "                classes = [\"abnormal\"]\n",
    "        else:\n",
    "            classes = [\"normal\", \"abnormal\"]\n",
    "        for c in classes:\n",
    "            patients_ids_positions = [\n",
    "                int(i.split(\"_\")[1].split(\"-\")[1].split(\".\")[0]) for i in sorted(\n",
    "                    os.listdir(os.path.join(base_path, m, c))\n",
    "                )\n",
    "            ]\n",
    "            all_data = \"all_{}_{}_patients\".format(t, c)\n",
    "            data = \"{}_{}_{}\".format(m, t, c)\n",
    "            key = \"{}_{}\".format(data, \"patients\")\n",
    "            globals()[key] = {}\n",
    "\n",
    "            for p_id in np.unique(patients_ids_positions):\n",
    "                globals()[key][p_id] = []\n",
    "                globals()[all_data][p_id] = []\n",
    "\n",
    "            for i, p_id in enumerate(patients_ids_positions):\n",
    "                globals()[key][p_id].append(globals()[data][i])\n",
    "                globals()[all_data][p_id].append(globals()[data][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norwegian-decline",
   "metadata": {},
   "source": [
    "##### Metrics loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optimum-genetics",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metrics = pd.read_csv(os.path.join(experiment_folder, \"metrics/train.csv\"))\n",
    "val_metrics = pd.read_csv(os.path.join(experiment_folder, \"metrics/val.csv\"))\n",
    "test_metrics = pd.read_csv(os.path.join(experiment_folder, \"metrics/test.csv\"))\n",
    "for i in [\"train\", \"val\", \"test\"]:\n",
    "    metric_file = globals()[\"{}_metrics\".format(i)]\n",
    "    index = metric_file.shape[0] - 1\n",
    "    print(\"{} metrics\".format(i))\n",
    "    print(\"AUC: {}\".format(metric_file.loc[index, \"auc\"]))\n",
    "    print(\"Acc: {}\".format(metric_file.loc[index, \"accuracy\"]))\n",
    "    print(\"Pre: {}\".format(metric_file.loc[index, \"precision\"]))\n",
    "    print(\"Rec: {}\".format(metric_file.loc[index, \"recall\"]))\n",
    "    print(\"Spe: {}\".format(metric_file.loc[index, \"specificity\"]))\n",
    "    print(\"F1: {}\".format(metric_file.loc[index, \"f1_score\"]))\n",
    "    print(\"=\"*15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d621ee",
   "metadata": {},
   "source": [
    "##### Selecting the threshold and calculating the standard metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9532a4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_table = []\n",
    "data_columns = [\"Partition\", \"Group\", \"AUC\", \"Threshold\", \"Acc\", \"Pre\", \"Rec\", \"Spe\", \"F1\"]\n",
    "\n",
    "errors = [\"normal\", \"abnormal\"]\n",
    "for t in [\"encoder\", \"contextual\", \"adversarial\"]:\n",
    "    for part in [\"val\", \"test\"]:\n",
    "        data = \"{}_{}_\".format(part, t)\n",
    "        y_true = np.concatenate([[i]*globals()[data + j].shape[0] for i,j in enumerate(errors)]) \n",
    "        y_pred = np.concatenate([globals()[data+i] for i in errors])\n",
    "\n",
    "        if part == \"val\":\n",
    "            precisions, recalls, thresholds = precision_recall_curve(y_true, y_pred)\n",
    "            deltas_pre_4_rec = np.abs(precisions - recalls)\n",
    "            threshold = thresholds[np.argmin(deltas_pre_4_rec[deltas_pre_4_rec != 0])]\n",
    "\n",
    "        if \"train_{}_normal\".format(t) in globals().keys():\n",
    "            y_pred = (y_pred > threshold).astype(np.int64)\n",
    "            test_class = \"normal\"\n",
    "        else:\n",
    "            y_pred = (y_pred < threshold).astype(np.int64)\n",
    "            test_class = \"abnormal\"\n",
    "\n",
    "        TP = tf.keras.metrics.TruePositives()\n",
    "        TN = tf.keras.metrics.TrueNegatives()\n",
    "        FP = tf.keras.metrics.FalsePositives()\n",
    "        FN = tf.keras.metrics.FalseNegatives()\n",
    "        AUC = tf.keras.metrics.AUC()\n",
    "\n",
    "        TP.update_state(y_true, y_pred)\n",
    "        TN.update_state(y_true, y_pred)\n",
    "        FP.update_state(y_true, y_pred)\n",
    "        FN.update_state(y_true, y_pred)\n",
    "        AUC.update_state(y_true, y_pred)\n",
    "\n",
    "        data_table.append([\n",
    "            part,\n",
    "            t,\n",
    "            round(AUC.result().numpy(), 3),\n",
    "            round(threshold, 3),\n",
    "            round(accuracy(TP.result().numpy(), TN.result().numpy(), FP.result().numpy(), FN.result().numpy()), 3),\n",
    "            round(precision(TP.result().numpy(), FP.result().numpy()), 3),\n",
    "            round(recall(TP.result().numpy(), FN.result().numpy()), 3),\n",
    "            round(specificity(TN.result().numpy(), FP.result().numpy()), 3),\n",
    "            round(f1_score(TP.result().numpy(), FP.result().numpy(), FN.result().numpy()), 3),\n",
    "        ])\n",
    "pd.DataFrame(data_table, columns=data_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7466b334",
   "metadata": {},
   "source": [
    "##### Calculating the homocedasticity metric by patient (individually)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879751ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_table = []\n",
    "data_columns = [\"Patient\", \"Belongs\", \"Against\", \"Group\", \"Brown\", \"Levene\", \"Barlett\"]\n",
    "test_class = \"normal\"\n",
    "\n",
    "for partition in [\"train\", \"val\", \"test\", \"all\"]:\n",
    "    for total_partition in [\"train\", \"val\", \"test\", \"all\"]:\n",
    "        for group in [\"encoder\", \"contextual\", \"adversarial\"]:\n",
    "            patients_dict = globals()[\"{}_{}_{}_patients\".format(partition, group, test_class)]\n",
    "            data_group = globals()[\"{}_{}_{}\".format(total_partition, group, test_class)]\n",
    "            for p_id in patients_dict:\n",
    "                data_table.append([\n",
    "                    p_id,\n",
    "                    partition,\n",
    "                    total_partition,\n",
    "                    group,\n",
    "                    int(brownForsythe_test(sorted(patients_dict[p_id]), sorted(data_group))),\n",
    "                    int(levene_test(sorted(patients_dict[p_id]), sorted(data_group))),\n",
    "                    int(bartlett_test(sorted(patients_dict[p_id]), sorted(data_group))),\n",
    "                ])\n",
    "\n",
    "df = pd.DataFrame(data_table, columns=data_columns)\n",
    "df.to_excel(os.path.join(experiment_folder, \"metrics/homocedasticity.xlsx\"), index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febab37a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.loc[\n",
    "    (df[\"Belongs\"] == \"val\") &\n",
    "#     (df[\"Against\"] == \"val\") &\n",
    "    (df[\"Group\"] == \"encoder\") \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c1e13f",
   "metadata": {},
   "source": [
    "##### Calculating the homocedasticity metric by partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbea7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_table = []\n",
    "data_columns = [\"Group\", \"Partition\", \"vs Part\", \"Homocedasticity level\"]\n",
    "test_class = \"normal\"\n",
    "\n",
    "for group in [\"encoder\", \"contextual\", \"adversarial\"]:\n",
    "    for partition in [\"train\", \"val\", \"test\", \"all\"]:\n",
    "        for total_partition in [\"train\", \"val\", \"test\", \"all\"]:\n",
    "            patients_dict = globals()[\"{}_{}_{}_patients\".format(partition, group, test_class)]\n",
    "            data_group = np.r_[sorted(globals()[\"{}_{}_{}\".format(total_partition, group, test_class)])]\n",
    "            homo_by_patients = []\n",
    "            for p_id in patients_dict:\n",
    "                homo_by_patients.append(\n",
    "                    int(brownForsythe_test(np.r_[sorted(patients_dict[p_id])], data_group))*0.5 + \n",
    "                    int(levene_test(np.r_[sorted(patients_dict[p_id])], data_group))*0.5\n",
    "                )\n",
    "            data_table.append([\n",
    "                group,\n",
    "                partition,\n",
    "                total_partition,\n",
    "                np.mean(homo_by_patients)\n",
    "            ])\n",
    "\n",
    "pd.DataFrame(data_table, columns=data_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "olympic-recipient",
   "metadata": {},
   "source": [
    "### Qualitative metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dynamic-remains",
   "metadata": {},
   "source": [
    "##### Dist. Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disciplinary-label",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_table = []\n",
    "data_columns = [\"Group\", \"Element\", \"Class\", \"Min\", \"Max\", \"Mean\", \"Std\", \"Ske\", \"Kur\", \"CDF(x > 0)\"]\n",
    "\n",
    "for g in [\"train\", \"val\", \"test\", \"all\"]:\n",
    "    if g == \"train\":\n",
    "        classes = [\"check\"]\n",
    "    else:\n",
    "        classes = [\"normal\", \"abnormal\"]\n",
    "    for cl in classes:\n",
    "        for t in [\"encoder\", \"contextual\", \"adversarial\"]:\n",
    "            if cl == \"check\":\n",
    "                if \"train_{}_normal\".format(t) in globals().keys():\n",
    "                    c = \"normal\"\n",
    "                else:\n",
    "                    c = \"abnormal\"\n",
    "            else:\n",
    "                c = cl\n",
    "            data = globals()[\"{}_{}_{}\".format(g, t, c)]\n",
    "            m = np.mean(data)\n",
    "            s = np.std(data)\n",
    "            data_table.append([\n",
    "                g,\n",
    "                t,\n",
    "                c,\n",
    "                np.min(data),\n",
    "                np.max(data),\n",
    "                m,\n",
    "                s,\n",
    "                stats.skew(data),\n",
    "                stats.kurtosis(data),\n",
    "                1 - stats.norm(m, s).cdf(0)\n",
    "            ])\n",
    "pd.DataFrame(data_table, columns=data_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opponent-status",
   "metadata": {},
   "source": [
    "##### Normality tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuing-installation",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_table = []\n",
    "data_columns = [\"Group\", \"Element\", \"Class\", \"Brow\", \"Lev\", \"Bar\", \"DAP\", \"AD\", \"SW\", \"Chi2\"]\n",
    "\n",
    "for g in [\"train\", \"val\", \"test\", \"all\"]:\n",
    "    if g == \"train\":\n",
    "        classes = [\"check\"]\n",
    "    else:\n",
    "        classes = [\"normal\", \"abnormal\"]\n",
    "    for cl in classes:\n",
    "        for t in [\"encoder\", \"contextual\", \"adversarial\"]:\n",
    "            if cl == \"check\":\n",
    "                if \"train_{}_normal\".format(t) in globals().keys():\n",
    "                    c = \"normal\"\n",
    "                else:\n",
    "                    c = \"abnormal\"\n",
    "            else:\n",
    "                c = cl\n",
    "            data = np.r_[sorted(globals()[\"{}_{}_{}\".format(g, t, c)])]\n",
    "            norm_dist = np.r_[sorted(stats.norm.rvs(loc=np.mean(data), scale=np.std(data), size=len(data), random_state=seed))]\n",
    "            chi_test = chiSquare_test(data, norm_dist)\n",
    "            data_table.append([\n",
    "                g,\n",
    "                t,\n",
    "                c,\n",
    "                int(brownForsythe_test(data, norm_dist)),\n",
    "                int(levene_test(data, norm_dist)),\n",
    "                int(bartlett_test(data, norm_dist)),\n",
    "                int(dagostinoPearson_test(data)),\n",
    "                int(andersonDarling_test(data)),\n",
    "                int(shapiroWilks_test(data)),\n",
    "                \"{} ({})\".format(int(chi_test[0]), round(chi_test[1], 5)),\n",
    "            ])\n",
    "pd.DataFrame(data_table, columns=data_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b995bd85",
   "metadata": {},
   "source": [
    "##### Chi Square Tests and Freedom Degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d8fc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_table = []\n",
    "data_columns = [\"Group\", \"Element\", \"Class\", \"DF\", \"Brow\", \"Lev\", \"Bar\", \"Norm\", \"Chi\", \"Chi2\"]\n",
    "\n",
    "for g in [\"train\", \"val\", \"test\", \"all\"]:\n",
    "    if g == \"train\":\n",
    "        classes = [\"check\"]\n",
    "    else:\n",
    "        classes = [\"normal\", \"abnormal\"]\n",
    "    for cl in classes:\n",
    "        for t in [\"encoder\", \"contextual\", \"adversarial\"]:\n",
    "            if cl == \"check\":\n",
    "                if \"train_{}_normal\".format(t) in globals().keys():\n",
    "                    c = \"normal\"\n",
    "                else:\n",
    "                    c = \"abnormal\"\n",
    "            else:\n",
    "                c = cl\n",
    "            data = np.r_[sorted(globals()[\"{}_{}_{}\".format(g, t, c)])]\n",
    "            for dfi in range(1, data.shape[0]):\n",
    "                norm_dist = np.r_[sorted(\n",
    "                    stats.norm.rvs(loc=np.mean(data), scale=np.std(data), size=data.shape, random_state=seed)\n",
    "                )]\n",
    "                chi_dist = np.r_[sorted(\n",
    "                    stats.chi.rvs(dfi, loc=np.mean(data), scale=np.std(data), size=data.shape, random_state=seed)\n",
    "                )]\n",
    "                chi2_dist = np.r_[sorted(\n",
    "                    stats.chi2.rvs(dfi, loc=np.mean(data), scale=np.std(data), size=data.shape, random_state=seed)\n",
    "                )]\n",
    "                try:\n",
    "                    norm_test = stats.chisquare(data, norm_dist, dfi)#chiSquare_test(data, norm_dist)\n",
    "                    norm_result = \"{} ({})\".format(int(norm_test.pvalue > 0.05), round(norm_test.statistic, 5))\n",
    "                except:\n",
    "                    norm_result = \"Invalid\"\n",
    "                try:\n",
    "                    chi_test = stats.chisquare(data, chi_dist, dfi)#chiSquare_test(data, chi_dist)\n",
    "                    chi_result = \"{} ({})\".format(int(chi_test.pvalue > 0.05), round(chi_test.statistic, 5))\n",
    "                except:\n",
    "                    chi_result = \"Invalid\"\n",
    "                try:\n",
    "                    chi2_test = chiSquare_test(data, chi2_dist)\n",
    "                    chi2_result = \"{} ({})\".format(int(chi2_test.pvalue > 0.05), round(chi2_test.statistic, 5))\n",
    "                except:\n",
    "                    chi2_result = \"Invalid\"\n",
    "                data_table.append([\n",
    "                    g,\n",
    "                    t,\n",
    "                    c,\n",
    "                    dfi, \n",
    "                    int(brownForsythe_test(data, norm_dist)),\n",
    "                    int(levene_test(data, norm_dist)),\n",
    "                    int(bartlett_test(data, norm_dist)),\n",
    "                    norm_result,\n",
    "                    chi_result,\n",
    "                    chi2_result\n",
    "                ])\n",
    "df = pd.DataFrame(data_table, columns=data_columns)\n",
    "df.to_excel(os.path.join(experiment_folder, \"metrics/distributions.xlsx\"), index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d1f3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Norm\"].unique(), df[\"Chi\"].unique(), df[\"Chi2\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41352f71",
   "metadata": {},
   "source": [
    "##### Grouping tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e810c92",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_table = []\n",
    "data_columns = [\"Element\", \"G1\", \"G2\", \"Brow\", \"Lev\", \"Bar\", \"MW\", \"KW\", \"KS\", \"Chi2 G1 -> G2\", \"Chi2 G2 -> G1\"]\n",
    "\n",
    "for t in [\"encoder\", \"contextual\", \"adversarial\"]:\n",
    "    for g1, g2 in [\n",
    "        (\"train\", \"val\"), (\"train\", \"test\"), (\"train\", \"all\"), \n",
    "        (\"val\", \"test\"), (\"val\", \"all\"),\n",
    "        (\"test\", \"all\")\n",
    "    ]:\n",
    "        if \"train_{}_normal\".format(t) in globals().keys():\n",
    "            c = \"normal\"\n",
    "        else:\n",
    "            c = \"abnormal\"\n",
    "        data1 = globals()[\"{}_{}_{}\".format(g1, t, c)]\n",
    "        data2 = globals()[\"{}_{}_{}\".format(g2, t, c)]\n",
    "        if data1.shape[0] > data2.shape[0]:\n",
    "            size = data2.shape[0] / data1.shape[0]\n",
    "            _, temporal = train_test_split(data1, test_size=size, random_state=seed)\n",
    "            sub_data1 = np.r_[sorted(temporal)]\n",
    "            sub_data2 = np.r_[sorted(data2)]\n",
    "        elif data1.shape[0] < data2.shape[0]:\n",
    "            size = data1.shape[0] / data2.shape[0]\n",
    "            _, temporal = train_test_split(data2, test_size=size, random_state=seed)\n",
    "            sub_data2 = np.r_[sorted(temporal)]\n",
    "            sub_data1 = np.r_[sorted(data1)]\n",
    "        else:\n",
    "            sub_data1 = data1\n",
    "            sub_data2 = data2\n",
    "        chi_test_g1 = chiSquare_test(sub_data1, sub_data2)\n",
    "        chi_test_g2 = chiSquare_test(sub_data2, sub_data1)\n",
    "        data1 = np.r_[sorted(data1)]\n",
    "        data2 = np.r_[sorted(data2)]\n",
    "        data_table.append([\n",
    "            t,\n",
    "            g1,\n",
    "            g2,\n",
    "            int(brownForsythe_test(data1, data2)),\n",
    "            int(levene_test(data1, data2)),\n",
    "            int(bartlett_test(data1, data2)),\n",
    "            int(mannWhitney_test(data1, data2)),\n",
    "            int(kruskalWallis_test(data1, data2)),\n",
    "            int(kolmogorovSmirnov_test(data1, data2)),\n",
    "            \"{} ({})\".format(int(chi_test_g1[0]), round(chi_test_g1[1], 5)),\n",
    "            \"{} ({})\".format(int(chi_test_g2[0]), round(chi_test_g2[1], 5)),\n",
    "        ])\n",
    "pd.DataFrame(data_table, columns=data_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5991a650",
   "metadata": {},
   "source": [
    "### Classing tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a34b1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_table = []\n",
    "data_columns = [\"Group\", \"Element\", \"Brow\", \"Lev\", \"Bar\", \"MW\", \"KW\", \"KS\", \"Chi2 N -> A\", \"Chi2 A -> N\"]\n",
    "for t in [\"encoder\", \"contextual\", \"adversarial\"]:\n",
    "    for g in [\"val\", \"test\", \"all\"]:\n",
    "        data1 = globals()[\"{}_{}_normal\".format(g, t)]\n",
    "        data2 = globals()[\"{}_{}_abnormal\".format(g, t)]\n",
    "        if data1.shape[0] > data2.shape[0]:\n",
    "            size = data2.shape[0] / data1.shape[0]\n",
    "            _, temporal = train_test_split(data1, test_size=size, random_state=seed)\n",
    "            sub_data1 = np.r_[sorted(temporal)]\n",
    "            sub_data2 = np.r_[sorted(data2)]\n",
    "        elif data1.shape[0] < data2.shape[0]:\n",
    "            size = data1.shape[0] / data2.shape[0]\n",
    "            _, temporal = train_test_split(data2, test_size=size, random_state=seed)\n",
    "            sub_data2 = np.r_[sorted(temporal)]\n",
    "            sub_data1 = np.r_[sorted(data1)]\n",
    "        else:\n",
    "            sub_data1 = np.r_[sorted(data1)]\n",
    "            sub_data2 = np.r_[sorted(data2)]\n",
    "        chi_test_1 = chiSquare_test(sub_data1, sub_data2)\n",
    "        chi_test_2 = chiSquare_test(sub_data2, sub_data1)\n",
    "        data1 = np.r_[sorted(data1)]\n",
    "        data2 = np.r_[sorted(data2)]\n",
    "        data_table.append([\n",
    "            g,\n",
    "            t, \n",
    "            int(brownForsythe_test(data1, data2)),\n",
    "            int(levene_test(data1, data2)),\n",
    "            int(bartlett_test(data1, data2)),\n",
    "            int(mannWhitney_test(data1, data2)),\n",
    "            int(kruskalWallis_test(data1, data2)),\n",
    "            int(kolmogorovSmirnov_test(data1, data2)),\n",
    "            \"{} ({})\".format(int(chi_test_1[0]), round(chi_test_1[1], 5)),\n",
    "            \"{} ({})\".format(int(chi_test_2[0]), round(chi_test_2[1], 5)),\n",
    "        ])\n",
    "pd.DataFrame(data_table, columns=data_columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
