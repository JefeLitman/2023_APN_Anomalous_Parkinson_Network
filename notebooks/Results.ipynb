{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "endangered-assets",
   "metadata": {},
   "source": [
    "# GANomaly Notebook for Individual Experiment Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embedded-billy",
   "metadata": {},
   "source": [
    "## Initial Configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protected-meter",
   "metadata": {},
   "source": [
    "### Libraries import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ignored-motorcycle",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-03 01:42:42.924667: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from ipywidgets import interact, IntSlider\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "sys.path.append(\"../\")\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bored-portuguese",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.metrics import accuracy, precision, recall, specificity, f1_score\n",
    "from utils.metrics import dagostinoPearson_test, andersonDarling_test, shapiroWilks_test, chiSquare_test, fOneWay_test\n",
    "from utils.metrics import brownForsythe_test, levene_test, bartlett_test\n",
    "from utils.metrics import mannWhitney_test, kruskalWallis_test, kolmogorovSmirnov_test\n",
    "from utils.savers import generate_qq_plot\n",
    "from utils.common import format_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5694e42d",
   "metadata": {},
   "source": [
    "### Functions definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b8c9648",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_curve(y_true, y_pred, num_thresholds=200):\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    thresholds = np.linspace(np.min(y_pred), np.max(y_pred), num_thresholds)\n",
    "    for t in thresholds:\n",
    "        tp = np.count_nonzero(np.logical_and(y_true, (y_pred > t)))\n",
    "        fp = np.count_nonzero(np.logical_and(np.logical_not(y_true), (y_pred > t)))\n",
    "        fn = np.count_nonzero(np.logical_and(y_true, (y_pred <= t)))\n",
    "        if tp+fp == 0:\n",
    "            precisions.append(0)\n",
    "        else:\n",
    "            precisions.append(precision(tp, fp))\n",
    "        if tp + fn == 0:\n",
    "            recalls.append(0)\n",
    "        else:\n",
    "            recalls.append(recall(tp, fn))\n",
    "    return np.r_[precisions], np.r_[recalls], thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eec12b3",
   "metadata": {},
   "source": [
    "### Experiments selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5551c490",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0001': {'path': '../results/Ganomaly_3D/0003_eval_of_0001/0001_Ganomaly3D-64x64x64x1'},\n",
       " '0002': {'path': '../results/Ganomaly_3D/0003_eval_of_0001/0002_Ganomaly3D-64x64x64x1'},\n",
       " '0003': {'path': '../results/Ganomaly_3D/0003_eval_of_0001/0003_Ganomaly3D-64x64x64x1'},\n",
       " '0004': {'path': '../results/Ganomaly_3D/0003_eval_of_0001/0004_Ganomaly3D-64x64x64x1'},\n",
       " '0005': {'path': '../results/Ganomaly_3D/0003_eval_of_0001/0005_Ganomaly3D-64x64x64x1'}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# experiments_ids = [\n",
    "#     \"0028\", \"0032\", \"0034\", \"0035\", # 3D B32 BN Control\n",
    "#     \"0038\", \"0042\", \"0047\", \"0052\", # 2D B32 BN Control\n",
    "#     \"0029\", \"0033\", \"0036\", \"0037\", # 3D B16 BN Control\n",
    "#     \"0039\", \"0043\", \"0050\", \"0055\", # 2D B16 BN Control\n",
    "#     \"0044\", \"0046\", \"0048\", \"0051\", # 3D B16 BN Parkinson\n",
    "#     \"0063\", \"0067\", \"0069\", \"0070\", # 2D B16 BN Parkinson\n",
    "#     \"0040\", \"0041\", \"0045\", \"0049\", # 3D B16 RGB Control\n",
    "#     \"0053\", \"0058\", \"0061\", \"0066\", # 2D B16 RGB Control\n",
    "#     \"0059\", \"0062\", \"0065\", \"0068\", # 3D B16 RGB Parkinson\n",
    "#     \"0054\", \"0057\", \"0060\", \"0064\", # 2D B16 RGB Parkinson\n",
    "# ]\n",
    "experiments_ids = [format_index(i) for i in range(1, 6)]\n",
    "# experiments_ids = [\"0029\", '0033', '0036', '0037']\n",
    "save_path_2_excels = \"../results/Results_Reports/\"\n",
    "\n",
    "exps = {}\n",
    "root_path = \"../results/Ganomaly_3D/0003_eval_of_0001/\"\n",
    "for exp_id in experiments_ids:\n",
    "    for i in sorted(os.listdir(root_path)):\n",
    "        if exp_id in i:\n",
    "            exp_path = os.path.join(root_path, i)\n",
    "            exps[exp_id] = {\n",
    "                \"path\": exp_path\n",
    "            }\n",
    "exps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308e4736",
   "metadata": {},
   "source": [
    "## Standar metrics excel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8092b9",
   "metadata": {},
   "source": [
    "### Errors loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "configured-tyler",
   "metadata": {},
   "outputs": [],
   "source": [
    "for exp_id in experiments_ids:\n",
    "    base_path = os.path.join(exps[exp_id][\"path\"], \"outputs/errors/\")\n",
    "    for t in [\"encoder\", \"contextual\", \"adversarial\"]:\n",
    "            for c in [\"normal\", \"abnormal\"]:\n",
    "                exps[exp_id][\"all_{}_{}\".format(t, c)] = np.r_[[]]\n",
    "            \n",
    "    for t in [\"encoder\", \"contextual\", \"adversarial\"]:\n",
    "        for m in [\"train\", \"test\"]:\n",
    "            if m == \"train\":\n",
    "                if os.path.isfile(os.path.join(base_path, t, m, \"normal.npy\")):\n",
    "                    classes = [\"normal\"] \n",
    "                else:\n",
    "                    classes = [\"abnormal\"]\n",
    "            else:\n",
    "                classes = [\"normal\", \"abnormal\"]\n",
    "\n",
    "            for c in classes:\n",
    "                all_data = \"all_{}_{}\".format(t, c)\n",
    "                errors = np.load(os.path.join(base_path, t, m, c + \".npy\"))\n",
    "                exps[exp_id][\"{}_{}_{}\".format(m, t, c)] = errors\n",
    "                exps[exp_id][all_data] = np.concatenate([exps[exp_id][all_data], errors])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10791fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for exp_id in experiments_ids:\n",
    "    base_path = os.path.join(exps[exp_id][\"path\"], \"outputs/latent_vectors/input_generator\")\n",
    "    for t in [\"encoder\", \"contextual\", \"adversarial\"]:\n",
    "        for m in [\"train\", \"test\"]:\n",
    "            if m == \"train\":\n",
    "                if \"train_encoder_normal\" in exps[exp_id].keys():\n",
    "                    classes = [\"normal\"] \n",
    "                else:\n",
    "                    classes = [\"abnormal\"]\n",
    "            else:\n",
    "                classes = [\"normal\", \"abnormal\"]\n",
    "            for c in classes:\n",
    "                patients_ids_positions = [\n",
    "                    int(i.split(\"_\")[1].split(\"-\")[1].split(\".\")[0]) for i in sorted(\n",
    "                        os.listdir(os.path.join(base_path, m, c))\n",
    "                    )\n",
    "                ]\n",
    "                data = \"{}_{}_{}\".format(m, t, c)\n",
    "                key = \"{}_{}\".format(data, \"patients\")\n",
    "                exps[exp_id][key] = {}\n",
    "                \n",
    "                for p_id in np.unique(patients_ids_positions):\n",
    "                    exps[exp_id][key][p_id] = []\n",
    "                \n",
    "                for i, p_id in enumerate(patients_ids_positions):\n",
    "                    exps[exp_id][key][p_id].append(exps[exp_id][data][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norwegian-decline",
   "metadata": {},
   "source": [
    "### Building excel with standard metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optimum-genetics",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_table = []\n",
    "data_columns = [\"Exp ID\", \"L_gen\", \"L_disc\", \"Acc (t=0.5)\", \"Pre_orig\", \n",
    "                \"Rec_orig\", \"Spe_orig\", \"F1_orig\", \"AUC\", \"Threshold\", \n",
    "                \"Acc_thre\", \"Pre_thre\", \"Rec_thre\", \"Spe_thre\", \"F1_thre\",\n",
    "                \"Homo Level\"\n",
    "               ]\n",
    "for exp_id in experiments_ids:\n",
    "    experiment_folder = exps[exp_id][\"path\"]\n",
    "    \n",
    "    train_metrics = pd.read_csv(os.path.join(experiment_folder, \"metrics/train.csv\"))\n",
    "    test_metrics = pd.read_csv(os.path.join(experiment_folder, \"metrics/test.csv\"))\n",
    "\n",
    "    group = \"encoder\"\n",
    "    data = \"test_{}_\".format(group)\n",
    "    errors = [\"normal\", \"abnormal\"]\n",
    "    y_true = np.concatenate([[i]*exps[exp_id][data + j].shape[0] for i,j in enumerate(errors)]) \n",
    "    y_pred = np.concatenate([exps[exp_id][data+i] for i in errors])\n",
    "    \n",
    "    if \"train_{}_normal\".format(group) in exps[exp_id].keys():\n",
    "        y_pred = (y_pred - np.min(y_pred)) / (np.max(y_pred) - np.min(y_pred))\n",
    "        test_class = \"normal\"\n",
    "    else:\n",
    "        y_pred = 1 - ((y_pred - np.min(y_pred)) / (np.max(y_pred) - np.min(y_pred)))\n",
    "        test_class = \"abnormal\"\n",
    "    \n",
    "    precisions, recalls, thresholds = precision_recall_curve(y_true, y_pred)\n",
    "    deltas_pre_4_rec = np.abs(precisions - recalls)\n",
    "    threshold = thresholds[np.argmin(deltas_pre_4_rec[deltas_pre_4_rec != 0])]\n",
    "\n",
    "    TP = tf.keras.metrics.TruePositives(threshold)\n",
    "    TN = tf.keras.metrics.TrueNegatives(threshold)\n",
    "    FP = tf.keras.metrics.FalsePositives(threshold)\n",
    "    FN = tf.keras.metrics.FalseNegatives(threshold)\n",
    "    AUC = tf.keras.metrics.AUC()\n",
    "\n",
    "    TP.update_state(y_true, y_pred)\n",
    "    TN.update_state(y_true, y_pred)\n",
    "    FP.update_state(y_true, y_pred)\n",
    "    FN.update_state(y_true, y_pred)\n",
    "    AUC.update_state(y_true, y_pred)\n",
    "    \n",
    "    homo_key = \"{}{}_patients\".format(data, test_class)\n",
    "    homo_metric = 0\n",
    "    for p_id in exps[exp_id][homo_key]:\n",
    "        homo_4_pat = int(brownForsythe_test(\n",
    "            sorted(exps[exp_id][homo_key][p_id]), sorted(exps[exp_id][\"train_{}_{}\".format(group, test_class)])\n",
    "        )) + int(levene_test(\n",
    "            sorted(exps[exp_id][homo_key][p_id]), sorted(exps[exp_id][\"train_{}_{}\".format(group, test_class)])\n",
    "        )) + int(bartlett_test(\n",
    "            sorted(exps[exp_id][homo_key][p_id]), sorted(exps[exp_id][\"train_{}_{}\".format(group, test_class)])\n",
    "        ))\n",
    "        homo_metric += homo_4_pat/3\n",
    "    homo_metric /= len(exps[exp_id][homo_key])\n",
    "    \n",
    "    data_table.append([\n",
    "        exp_id,\n",
    "        train_metrics.loc[train_metrics.shape[0] - 1 ,\"gen_error\"],\n",
    "        train_metrics.loc[train_metrics.shape[0] - 1 ,\"disc_error\"],\n",
    "        test_metrics.loc[test_metrics.shape[0] - 1 ,\"accuracy\"],\n",
    "        test_metrics.loc[test_metrics.shape[0] - 1 ,\"precision\"],\n",
    "        test_metrics.loc[test_metrics.shape[0] - 1 ,\"recall\"],\n",
    "        test_metrics.loc[test_metrics.shape[0] - 1 ,\"specificity\"],\n",
    "        test_metrics.loc[test_metrics.shape[0] - 1 ,\"f1_score\"],\n",
    "        AUC.result().numpy(),\n",
    "        threshold,\n",
    "        np.max(accuracy(TP.result().numpy(), TN.result().numpy(), FP.result().numpy(), FN.result().numpy())),\n",
    "        np.max(precision(TP.result().numpy(), FP.result().numpy())),\n",
    "        np.max(recall(TP.result().numpy(), FN.result().numpy())),\n",
    "        np.max(specificity(TN.result().numpy(), FP.result().numpy())),\n",
    "        np.max(f1_score(TP.result().numpy(), FP.result().numpy(), FN.result().numpy())),\n",
    "        homo_metric\n",
    "    ])\n",
    "\n",
    "pd.DataFrame(data_table, columns=data_columns).to_excel(os.path.join(save_path_2_excels, \"standard_metrics.xlsx\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "100e1fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3055555555555555\n",
      "0.027777777777777776\n",
      "0.08333333333333333\n"
     ]
    }
   ],
   "source": [
    "# Testing homocedasticity methods\n",
    "#group = \"contextual\"\n",
    "exp_id = \"0001\"\n",
    "test_class = \"normal\"\n",
    "compare_group = \"train\"\n",
    "\n",
    "for group in [\"encoder\", \"contextual\", \"adversarial\"]:\n",
    "    data = \"train_{}_\".format(group)\n",
    "    homo_key = \"{}{}_patients\".format(data, test_class)\n",
    "    homo_metric = 0\n",
    "    for p_id in exps[exp_id][homo_key]:\n",
    "        homo_4_pat = int(brownForsythe_test(\n",
    "            sorted(exps[exp_id][homo_key][p_id]), sorted(exps[exp_id][\"{}_{}_{}\".format(compare_group, group, test_class)])\n",
    "        )) + int(levene_test(\n",
    "            sorted(exps[exp_id][homo_key][p_id]), sorted(exps[exp_id][\"{}_{}_{}\".format(compare_group, group, test_class)])\n",
    "        )) + int(bartlett_test(\n",
    "            sorted(exps[exp_id][homo_key][p_id]), sorted(exps[exp_id][\"{}_{}_{}\".format(compare_group, group, test_class)])\n",
    "        ))\n",
    "        homo_metric += homo_4_pat/3\n",
    "#         print(homo_4_pat)\n",
    "    homo_metric /= len(exps[exp_id][homo_key])\n",
    "    print(homo_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ec89c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in [\"encoder\", \"contextual\", \"adversarial\"]:\n",
    "        for c in [\"normal\", \"abnormal\"]:\n",
    "            globals()[\"total_{}_{}\".format(t, c)] = np.r_[[]]\n",
    "\n",
    "for exp_id in experiments_ids:\n",
    "    for t in [\"encoder\", \"contextual\", \"adversarial\"]:\n",
    "#         for m in [\"train\", \"test\"]:\n",
    "#             if m == \"train\":\n",
    "#                 if \"train_encoder_normal\" in exps[exp_id].keys():\n",
    "#                     classes = [\"normal\"] \n",
    "#                 else:\n",
    "#                     classes = [\"abnormal\"]\n",
    "#             else:\n",
    "#                 classes = [\"normal\", \"abnormal\"]\n",
    "        m = \"test\"\n",
    "        for c in [\"normal\", \"abnormal\"]:\n",
    "            total_data = \"total_{}_{}\".format(t, c)\n",
    "            globals()[total_data] = np.concatenate([\n",
    "                globals()[total_data], \n",
    "                exps[exp_id][\"{}_{}_{}\".format(m, t, c)]\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "71c3eda7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(313,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_encoder_abnormal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dda57f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "exps['0037']['test_encoder_abnormal'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1c3be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_encoder_abnormal = np.array([0]*88, dtype=np.float64)\n",
    "# for i in experiments_ids:\n",
    "#     total_encoder_abnormal += exps[i][\"test_encoder_abnormal\"]\n",
    "# total_encoder_abnormal /= len(experiments_ids)\n",
    "# total_encoder_abnormal = exps['0036'][\"test_encoder_abnormal\"]\n",
    "total_encoder_abnormal = np.concatenate([\n",
    "    exps['0029'][\"test_encoder_abnormal\"][:24],\n",
    "    exps['0033'][\"test_encoder_abnormal\"][24:48],\n",
    "    exps['0036'][\"test_encoder_abnormal\"][48:72],\n",
    "    exps['0037'][\"test_encoder_abnormal\"][72:88]\n",
    "])\n",
    "total_encoder_abnormal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7b0fec96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3996993503977905\t0.7063252925872803\t0.6915584206581116\t0.7191011309623718\t0.6805111765861511\t0.7293447256088257\t0.6859903381642513\t0.7049279808998108\n"
     ]
    }
   ],
   "source": [
    "group = \"encoder\"\n",
    "data = \"total_{}_\".format(group)\n",
    "errors = [\"normal\", \"abnormal\"]\n",
    "y_true = np.concatenate([[i]*globals()[data + j].shape[0] for i,j in enumerate(errors)]) \n",
    "y_pred = np.concatenate([globals()[data + i] for i in errors])\n",
    "\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_true, y_pred)\n",
    "deltas_pre_4_rec = np.abs(precisions - recalls)\n",
    "threshold = thresholds[np.argmin(deltas_pre_4_rec[deltas_pre_4_rec != 0])]\n",
    "\n",
    "path = \"../results/Results_Reports/\"\n",
    "plt.plot(thresholds, precisions, label=\"Precision\")\n",
    "plt.plot(thresholds, recalls, label=\"Recall\")\n",
    "plt.axvline(threshold, color=\"black\", alpha=0.5)\n",
    "plt.title('Precision and Recall for different thresholds')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Precision/Recall')\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(path, \"Precision_vs_Recall.png\"),dpi=600)\n",
    "plt.close()\n",
    "\n",
    "if True:#\"train_{}_normal\".format(group) in globals().keys():\n",
    "    y_pred = (y_pred > threshold).astype(np.int64)\n",
    "    test_class = \"normal\"\n",
    "else:\n",
    "    #y_true = 1 - y_true\n",
    "    y_pred = (y_pred < threshold).astype(np.int64)\n",
    "    test_class = \"abnormal\"\n",
    "#y_pred = (y_pred > threshold).astype(np.int64)\n",
    "\n",
    "TP = tf.keras.metrics.TruePositives()\n",
    "TN = tf.keras.metrics.TrueNegatives()\n",
    "FP = tf.keras.metrics.FalsePositives()\n",
    "FN = tf.keras.metrics.FalseNegatives()\n",
    "AUC = tf.keras.metrics.AUC()\n",
    "\n",
    "TP.update_state(y_true, y_pred)\n",
    "TN.update_state(y_true, y_pred)\n",
    "FP.update_state(y_true, y_pred)\n",
    "FN.update_state(y_true, y_pred)\n",
    "AUC.update_state(y_true, y_pred)\n",
    "\n",
    "print(\"{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\".format(\n",
    "    threshold,\n",
    "    accuracy(TP.result().numpy(), TN.result().numpy(), FP.result().numpy(), FN.result().numpy()),\n",
    "    precision(TP.result().numpy(), FP.result().numpy()),\n",
    "    TN.result().numpy() / (TN.result().numpy() + FN.result().numpy()),\n",
    "    recall(TP.result().numpy(), FN.result().numpy()),\n",
    "    specificity(TN.result().numpy(), FP.result().numpy()),\n",
    "    f1_score(TP.result().numpy(), FP.result().numpy(), FN.result().numpy()),\n",
    "    AUC.result().numpy()\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "olympic-recipient",
   "metadata": {},
   "source": [
    "## Qualitative metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dynamic-remains",
   "metadata": {},
   "source": [
    "### Errors exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "disciplinary-label",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_table = []\n",
    "data_columns = [\"Exp ID\", \"Min\", \"Max\", \"Mean\", \"Std\", \"Ske\", \"Kur\", \"CDF (x > 0)\"]\n",
    "\n",
    "for g in [\"train\", \"test\", \"all\"]:\n",
    "        if g == \"train\":\n",
    "            classes = [\"check\"]\n",
    "        else:\n",
    "            classes = [\"normal\", \"abnormal\"]\n",
    "        for cl in classes:\n",
    "            for t in [\"encoder\", \"contextual\", \"adversarial\"]:\n",
    "                \n",
    "                if cl == \"check\":\n",
    "                    data_table.append([\"{} {}\".format(g, t)] + [None]*5)\n",
    "                else:\n",
    "                    data_table.append([\"{} {} {}\".format(g, t, cl)] + [None]*5)\n",
    "                    \n",
    "                for exp_id in experiments_ids:\n",
    "                    if cl == \"check\":\n",
    "                        if \"train_{}_normal\".format(t) in exps[exp_id].keys():\n",
    "                            c = \"normal\"\n",
    "                        else:\n",
    "                            c = \"abnormal\"\n",
    "                    else:\n",
    "                        c = cl\n",
    "                    \n",
    "                    data = exps[exp_id][\"{}_{}_{}\".format(g, t, c)]\n",
    "                    m = np.mean(data)\n",
    "                    s = np.std(data)\n",
    "                \n",
    "                    data_table.append([\n",
    "                        exp_id,\n",
    "                        np.min(data),\n",
    "                        np.max(data),\n",
    "                        m,\n",
    "                        s,\n",
    "                        stats.skew(data),\n",
    "                        stats.kurtosis(data),\n",
    "                        1 - stats.norm(m, s).cdf(0)\n",
    "                    ])\n",
    "pd.DataFrame(data_table, columns=data_columns).to_excel(\n",
    "    os.path.join(save_path_2_excels, \"qualitative_metrics.xlsx\"), \n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opponent-status",
   "metadata": {},
   "source": [
    "### Normality tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "continuing-installation",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_table = []\n",
    "data_columns = [\"Exp ID\", \"Brown\", \"Levene\", \"Barlett\", \"DAP\", \"AD\", \"SW\", \"Chi^2\", \"F test\"]\n",
    "\n",
    "for g in [\"train\", \"test\", \"all\"]:\n",
    "    if g == \"train\":\n",
    "        classes = [\"check\"]\n",
    "    else:\n",
    "        classes = [\"normal\", \"abnormal\"]\n",
    "    for cl in classes:\n",
    "        for t in [\"encoder\", \"contextual\", \"adversarial\"]:\n",
    "\n",
    "            if cl == \"check\":\n",
    "                data_table.append([\"{} {}\".format(g, t)] + [None]*8)\n",
    "            else:\n",
    "                data_table.append([\"{} {} {}\".format(g, t, cl)] + [None]*8)\n",
    "\n",
    "            for exp_id in experiments_ids:\n",
    "                if cl == \"check\":\n",
    "                    if \"train_{}_normal\".format(t) in exps[exp_id].keys():\n",
    "                        c = \"normal\"\n",
    "                    else:\n",
    "                        c = \"abnormal\"\n",
    "                else:\n",
    "                    c = cl\n",
    "\n",
    "                data = sorted(exps[exp_id][\"{}_{}_{}\".format(g, t, c)])\n",
    "                norm_dist = sorted(stats.norm.rvs(loc=np.mean(data), scale=np.std(data), size=len(data), random_state=8128))\n",
    "                chi_test = chiSquare_test(data, norm_dist)\n",
    "                data_table.append([\n",
    "                    exp_id,\n",
    "                    int(brownForsythe_test(data, norm_dist)),\n",
    "                    int(levene_test(data, norm_dist)),\n",
    "                    int(bartlett_test(data, norm_dist)),\n",
    "                    int(dagostinoPearson_test(data)),\n",
    "                    int(andersonDarling_test(data)),\n",
    "                    int(shapiroWilks_test(data)),\n",
    "                    \"{} ({})\".format(int(chi_test[0]), round(chi_test[1], 5)),\n",
    "                    int(fOneWay_test(data, norm_dist))\n",
    "                ])\n",
    "pd.DataFrame(data_table, columns=data_columns).to_excel(\n",
    "    os.path.join(save_path_2_excels, \"normality_tests.xlsx\"), \n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41352f71",
   "metadata": {},
   "source": [
    "### Grouping tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2e810c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_table = []\n",
    "data_columns = [\"Exp ID\", \"Brown\", \"Levene\", \"Barlett\", \"MW\", \"KW\", \"KS\", \"F test\"]\n",
    "\n",
    "for t in [\"encoder\", \"contextual\", \"adversarial\"]:\n",
    "    for g1, g2 in [(\"train\", \"test\"), (\"test\", \"all\"), (\"train\", \"all\")]:\n",
    "        data_table.append([\"{} {} vs {}\".format(t, g1, g2)] + [None]*7)\n",
    "        for exp_id in experiments_ids:\n",
    "            if \"train_{}_normal\".format(t) in exps[exp_id].keys():\n",
    "                c = \"normal\"\n",
    "            else:\n",
    "                c = \"abnormal\"\n",
    "                \n",
    "            data1 = sorted(exps[exp_id][\"{}_{}_{}\".format(g1, t, c)])\n",
    "            data2 = sorted(exps[exp_id][\"{}_{}_{}\".format(g2, t, c)])\n",
    "            \n",
    "            data_table.append([\n",
    "                exp_id,\n",
    "                int(brownForsythe_test(data1, data2)),\n",
    "                int(levene_test(data1, data2)),\n",
    "                int(bartlett_test(data1, data2)),\n",
    "                int(mannWhitney_test(data1, data2)),\n",
    "                int(kruskalWallis_test(data1, data2)),\n",
    "                int(kolmogorovSmirnov_test(data1, data2)),\n",
    "                int(fOneWay_test(data1, data2))\n",
    "            ])\n",
    "pd.DataFrame(data_table, columns=data_columns).to_excel(\n",
    "    os.path.join(save_path_2_excels, \"grouping_tests.xlsx\"), \n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5991a650",
   "metadata": {},
   "source": [
    "### Classing tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3a34b1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_table = []\n",
    "data_columns = [\"Exp ID\", \"Brown\", \"Levene\", \"Barlett\", \"Chi^2 N->A\", \"Chi^2 A->N\", \"MW\", \"KW\", \"KS\", \"F test\"]\n",
    "\n",
    "for g in [\"test\", \"all\"]:\n",
    "    for t in [\"encoder\", \"contextual\", \"adversarial\"]:\n",
    "        data_table.append([\"{} {}\".format(g, t)] + [None]*9)\n",
    "        for exp_id in experiments_ids:\n",
    "            data1 = sorted(exps[exp_id][\"{}_{}_normal\".format(g, t)])\n",
    "            data2 = sorted(exps[exp_id][\"{}_{}_abnormal\".format(g, t)])\n",
    "            \n",
    "            row = [\n",
    "                exp_id,\n",
    "                int(brownForsythe_test(data1, data2)),\n",
    "                int(levene_test(data1, data2)),\n",
    "                int(bartlett_test(data1, data2)),\n",
    "            ]\n",
    "            if g == \"test\":\n",
    "                row += [\n",
    "                    int(mannWhitney_test(data1, data2)),\n",
    "                    int(kruskalWallis_test(data1, data2)),\n",
    "                    int(kolmogorovSmirnov_test(data1, data2)),\n",
    "                    int(fOneWay_test(data1, data2)),\n",
    "                    None,\n",
    "                    None\n",
    "                ]\n",
    "            else:\n",
    "                chi_1_test = chiSquare_test(data1, data2)\n",
    "                chi_2_test = chiSquare_test(data2, data1)\n",
    "                row += [\n",
    "                    \"{} ({})\".format(int(chi_1_test[0]), round(chi_1_test[1], 5)),\n",
    "                    \"{} ({})\".format(int(chi_2_test[0]), round(chi_2_test[1], 5)),\n",
    "                    int(mannWhitney_test(data1, data2)),\n",
    "                    int(kruskalWallis_test(data1, data2)),\n",
    "                    int(kolmogorovSmirnov_test(data1, data2)),\n",
    "                    int(fOneWay_test(data1, data2))\n",
    "                ]\n",
    "            \n",
    "            data_table.append(row)\n",
    "        \n",
    "pd.DataFrame(data_table, columns=data_columns).to_excel(\n",
    "    os.path.join(save_path_2_excels, \"classing_tests.xlsx\"), \n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "dcdadcf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder tests (normal):\t1\t0\t1\t0\t0\t0\t1 (26.59235)\t1\n",
      "\n",
      "contextual tests (normal):\t1\t1\t1\t0\t0\t0\t1 (2.91873)\t1\n",
      "\n",
      "adversarial tests (normal):\t0\t0\t1\t0\t0\t0\t1 (56.9364)\t1\n",
      "\n",
      "encoder tests (abnormal):\t0\t1\t1\t0\t0\t0\t1 (75.33484)\t1\n",
      "\n",
      "contextual tests (abnormal):\t1\t1\t1\t1\t0\t0\t1 (0.84015)\t1\n",
      "\n",
      "adversarial tests (abnormal):\t1\t1\t1\t0\t0\t0\t1 (12.09595)\t1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Normality test for total data\n",
    "for c in [\"normal\", \"abnormal\"]:\n",
    "    for t in [\"encoder\", \"contextual\", \"adversarial\"]:\n",
    "        data = sorted(globals()[\"total_{}_{}\".format(t, c)])\n",
    "        norm_dist = sorted(stats.norm.rvs(loc=np.mean(data), scale=np.std(data), size=len(data), random_state=8128))\n",
    "        chi_test = chiSquare_test(data, norm_dist)\n",
    "        print(\"{} tests ({}):\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\".format(\n",
    "            t,\n",
    "            c,\n",
    "            int(brownForsythe_test(data, norm_dist)),\n",
    "            int(levene_test(data, norm_dist)),\n",
    "            int(bartlett_test(data, norm_dist)),\n",
    "            int(dagostinoPearson_test(data)),\n",
    "            int(andersonDarling_test(data)),\n",
    "            int(shapiroWilks_test(data)),\n",
    "            \"{} ({})\".format(int(chi_test[0]), round(chi_test[1], 5)),\n",
    "            int(fOneWay_test(data, norm_dist))\n",
    "        ))\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a78c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping test for total data \n",
    "for t in [\"encoder\", \"contextual\", \"adversarial\"]:\n",
    "    print(\"-------------- {} ---------------------\".format(t))\n",
    "    for g1, g2 in [(\"train\", \"test\"), (\"test\", \"all\"), (\"train\", \"all\")]:\n",
    "        if \"train_{}_normal\".format(t) in globals().keys():\n",
    "            c = \"normal\"\n",
    "        else:\n",
    "            c = \"abnormal\"\n",
    "        data1 = sorted(globals()[\"{}_{}_{}\".format(g1, t, c)])\n",
    "        data2 = sorted(globals()[\"{}_{}_{}\".format(g2, t, c)])\n",
    "        print(\"{} vs {}: {}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\".format(\n",
    "            g1,\n",
    "            g2,\n",
    "            int(brownForsythe_test(data1, data2)),\n",
    "            int(levene_test(data1, data2)),\n",
    "            int(bartlett_test(data1, data2)),\n",
    "            int(mannWhitney_test(data1, data2)),\n",
    "            int(kruskalWallis_test(data1, data2)),\n",
    "            int(kolmogorovSmirnov_test(data1, data2)),\n",
    "            int(fOneWay_test(data1, data2))\n",
    "        ))\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e72088",
   "metadata": {},
   "source": [
    "## Visual results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a09b3d4",
   "metadata": {},
   "source": [
    "### Loading latent vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c14c5f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for exp_id in experiments_ids:\n",
    "    base_path = os.path.join(exps[exp_id][\"path\"], \"outputs/latent_vectors/\")\n",
    "    for n in [\"generator\", \"discriminator\"]:\n",
    "        for t in [\"input\", \"output\"]:\n",
    "            for c in [\"normal\", \"abnormal\"]:\n",
    "                exps[exp_id][\"all_{}_{}_{}\".format(n, t, c)] = []\n",
    "    \n",
    "    for n in [\"generator\", \"discriminator\"]:\n",
    "        for t in [\"input\", \"output\"]:\n",
    "            for m in [\"train\", \"test\"]:\n",
    "                if m == \"train\":\n",
    "                    if len(os.listdir(os.path.join(base_path, t + \"_\" + n, m, \"normal\"))) != 0:\n",
    "                        classes = [\"normal\"] \n",
    "                    elif len(os.listdir(os.path.join(base_path, t + \"_\" + n, m, \"abnormal\"))) != 0:\n",
    "                        classes = [\"abnormal\"]\n",
    "                    else:\n",
    "                        raise AssertionError(\"There are no vectors for the train folders in the experiment {}\".format(exp_id))\n",
    "                else:\n",
    "                    classes = [\"normal\", \"abnormal\"]\n",
    "                for c in classes:\n",
    "                    all_data = \"all_{}_{}_{}\".format(n, t, c)\n",
    "                    data = \"{}_{}_{}_{}\".format(m, n, t, c)\n",
    "                    exps[exp_id][data] = []\n",
    "                    path = os.path.join(base_path, t + \"_\" + n, m, c)\n",
    "                    for file in sorted(os.listdir(path)):\n",
    "                        vector = np.load(os.path.join(path, file))\n",
    "                        exps[exp_id][data].append(vector)\n",
    "                        exps[exp_id][all_data].append(vector)\n",
    "                    exps[exp_id][data] = np.r_[exps[exp_id][data]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1a415a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a6b571",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
