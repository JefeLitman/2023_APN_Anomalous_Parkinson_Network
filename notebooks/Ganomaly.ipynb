{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "endangered-assets",
   "metadata": {},
   "source": [
    "# GANomaly Notebook Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embedded-billy",
   "metadata": {},
   "source": [
    "## Initial Configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sensitive-blond",
   "metadata": {},
   "source": [
    "### Selecting the device to work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "micro-generator",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protected-meter",
   "metadata": {},
   "source": [
    "### Libraries import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ignored-motorcycle",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "import gc\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from IPython.display import clear_output\n",
    "\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bored-portuguese",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.ganomaly.model import get_2D_models, get_3D_models\n",
    "from models.ganomaly.utils.losses import l1_loss, l2_loss, BCELoss\n",
    "from models.ganomaly.utils.preprocessing import normalize_accros_channels, min_max_scaler, resize, get_center_of_volume, undo_enumerate\n",
    "from models.ganomaly.utils.weights_init import reinit_model\n",
    "from models.ganomaly.utils.exp_docs import experiment_folder_path, get_metrics_path, get_outputs_path\n",
    "from datasets.dict_features import get_parkinson\n",
    "from utils.metrics import get_true_positives, get_true_negatives, get_false_positives, get_false_negatives, accuracy, precision, recall, specificity, f1_score, get_AUC,shapiroWilks_test, dagostinoPearson_test, bartlett_test, levene_test, fOneWay_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "significant-holmes",
   "metadata": {},
   "source": [
    "### GPU Memory Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subjective-diary",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.getenv(\"CUDA_VISIBLE_DEVICES\") != '-1':\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "tf.debugging.set_log_device_placement(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "labeled-audit",
   "metadata": {},
   "source": [
    "## Dataset Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norwegian-decline",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optimum-genetics",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CPUS = 16\n",
    "dataset_path = \"/data/Datasets/parkinson/tf_records/data2020_cuttedFrames/parkinson_2020_cutted.tfrecord\"\n",
    "encoding_dictionary = get_parkinson()\n",
    "encoding_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "covered-startup",
   "metadata": {},
   "source": [
    "### General extraction function for tfrecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "packed-shanghai",
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_bytes_to_dict(example_bytes, encoding_dictionary):\n",
    "    return tf.io.parse_single_example(example_bytes, encoding_dictionary)\n",
    "\n",
    "def extract_data_from_dict(example_dict):\n",
    "    f = example_dict[\"frames\"]\n",
    "    h = example_dict[\"height\"]\n",
    "    w = example_dict[\"width\"]\n",
    "    c = example_dict[\"channels\"]\n",
    "    raw_volume = tf.io.decode_raw(example_dict[\"video\"], tf.uint8)\n",
    "    volume = tf.reshape(raw_volume, [f,h,w,c])\n",
    "    return tf.cast(volume, dtype=tf.float32), example_dict[\"parkinson\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arranged-spirit",
   "metadata": {},
   "source": [
    "### Data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "referenced-frederick",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = tf.data.TFRecordDataset(dataset_path)\n",
    "dict_data = raw_data.map(lambda x: from_bytes_to_dict(x, encoding_dictionary), N_CPUS)\n",
    "total_data = dict_data.map(extract_data_from_dict, N_CPUS)\n",
    "\n",
    "resize_data = total_data.map(lambda x,y: resize(x, y, [64,64]), N_CPUS)\n",
    "resize_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reported-history",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_videos = []\n",
    "labels_videos = []\n",
    "for x, y in resize_data:\n",
    "    shape_videos.append(x.numpy().shape)\n",
    "    labels_videos.append(y.numpy())\n",
    "shape_videos = np.r_[shape_videos]\n",
    "labels_videos = np.r_[labels_videos]\n",
    "print(\"Data information about the data\")\n",
    "print(\"Total videos: \", shape_videos.shape[0])\n",
    "print(\"Min value of frames: \", np.min(shape_videos[:,0]))\n",
    "print(\"Max value of frames: \", np.max(shape_videos[:,0]))\n",
    "print(\"Mean value of frames: \", np.mean(shape_videos[:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brave-respondent",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_data = resize_data.map(lambda x, y: get_center_of_volume(x, y, 64), N_CPUS)\n",
    "normalized_data = sampled_data.map(lambda x,y: normalize_accros_channels(x, y, 0.5, 0.5), N_CPUS)\n",
    "scaled_data = normalized_data.map(lambda x, y: min_max_scaler(x, y, -1., 1.), N_CPUS)\n",
    "\n",
    "\n",
    "normal_data = scaled_data.filter(lambda x,y: tf.equal(y, 0))\n",
    "abnormal_data = scaled_data.filter(lambda x,y: tf.equal(y, 1))\n",
    "normal_data, abnormal_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chief-hours",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import display, clear_output\n",
    "# import matplotlib.pyplot as plt\n",
    "# from ipywidgets import interact, IntSlider\n",
    "\n",
    "# x = x.numpy().astype(np.uint8)\n",
    "\n",
    "# interact(lambda frame: plt.imshow(x[frame-1]), frame=IntSlider(min=1, max=x.shape[0], step=1));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superior-criterion",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tracked-minimum",
   "metadata": {},
   "source": [
    "### Model params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regular-detail",
   "metadata": {},
   "outputs": [],
   "source": [
    "isize = 64 # Input size of the data (image or volume)\n",
    "nz = 100 # Context vector size\n",
    "nc = 3 # Quantity of channels in the data\n",
    "ngf = 64 # Quantity of initial filters in the first convolution of the encoder\n",
    "extra_layers = 0 # Quantity of layer blocks to add before reduction\n",
    "w_gen = (1, 50, 1) # Tuple with 3 elements (w_adv, w_con, w_enc) to use in the error of generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abroad-opportunity",
   "metadata": {},
   "source": [
    "### Experiment params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dress-wyoming",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dimension = \"3D\" # Dimension of model to use in experiment\n",
    "batch_size = 16 # Size of the bath for the model\n",
    "epochs = 15000 # Quantity of epochs to do in the training\n",
    "beta_1 = 0.5 # Momentum of beta 1 in adam optimizer for generator and discriminator\n",
    "beta_2 = 0.999 # Momentum of beta 2 in adam optimizer for generator and discriminator\n",
    "lr = 0.0002 # Initial learning rate for adam optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "active-safety",
   "metadata": {},
   "source": [
    "### Replicability configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacterial-things",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(8128)\n",
    "np.random.seed(8128)\n",
    "tf.random.set_seed(8128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empty-zoning",
   "metadata": {},
   "source": [
    "### Experiment documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streaming-technique",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_path, experiment_id = experiment_folder_path(\"/home/jefelitman/Overleaf_Server/\", model_dimension, isize, nc)\n",
    "\n",
    "# Metrics folder for model graphs\n",
    "metric_save_path = get_metrics_path(experiment_path)\n",
    "\n",
    "# Output folder for outputs\n",
    "outputs_path = get_outputs_path(experiment_path)\n",
    "experiment_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hydraulic-space",
   "metadata": {},
   "outputs": [],
   "source": [
    "readme = open(os.path.join(experiment_path, \"README.txt\"), \"w+\")\n",
    "readme.write(\n",
    "\"\"\"This file contains information about the experiment made in this instance.\n",
    "\n",
    "All models saved don't include the optimizer, but this file explains how to train in the same conditions.\n",
    "\n",
    "Basic notation:\n",
    "- {i}_Ganomaly_{d}: Experiment id, name of the model and operation dimensionality of convolutions.\n",
    "- H x W x C or F x H x W x C: Data dimensions used where F are frames, H height, W width and C channels.\n",
    "\n",
    "Experiment settings:\n",
    "- The seed used was 8128 for python random module, numpy random and tf random after the library importations.\n",
    "- The batch size was of {batch}.\n",
    "- The optimizer used in this experiment was Adam for generator and discriminator.\n",
    "- The number of classes in this dataset are 2 (Normal and Parkinson) .\n",
    "- This experiment use the data of parkinson_2020_cutted.tfrecord from data2020_cuttedFrames data.\n",
    "- The initial lr was of {lr}.\n",
    "- The beta 1 and beta 2 for adam optimizer was {beta_1} and {beta_2} respectively.\n",
    "- The total epochs made in this experiment was of {epochs}.\n",
    "- The context vector size (nz) was of {nz}.\n",
    "- The # channels in data (nc) was of {nc}.\n",
    "- The initial filters in the first convolution of the encoder was {ngf}.\n",
    "- The quantity of layer blocks to add before reduction was of {extra_layers}.\n",
    "- The weights for adversarial, contextual and encoder error in generator was {w_gen}.\n",
    "\n",
    "Transformations applied to data (following this order):\n",
    "- Resize: We resize the frames of volumes to H x W (64 x 64).\n",
    "- Centered volume: We take 64 frames on the center of volume to train and test the data.\n",
    "- Normalize: We normalize the volume with mean and std of 0.5 for both.\n",
    "- Scale: We scale the data between -1 and 1 using min max scaler to be comparable with generated images.\n",
    "- Randomize: We randomize the order of samples in every epoch.\n",
    "\n",
    "Training process:\n",
    "- The data doesn't have train and test partition but we make the partitions like this:\n",
    "    * 75% of normal data is used in train randomly selected.\n",
    "    * 25% of normal data is used in test randomly selected.\n",
    "    * 100% of abnormal (parkinson) data is used in test.\n",
    "\"\"\".format(\n",
    "        i = experiment_id,\n",
    "        d = model_dimension,\n",
    "        batch = batch_size,\n",
    "        lr = lr,\n",
    "        beta_1 = beta_1,\n",
    "        beta_2 = beta_2,\n",
    "        epochs = epochs,\n",
    "        nz = nz,\n",
    "        nc = nc,\n",
    "        ngf = ngf,\n",
    "        extra_layers = extra_layers,\n",
    "        w_gen = w_gen\n",
    "    )\n",
    ")\n",
    "readme.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "associate-robert",
   "metadata": {},
   "source": [
    "### Models creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunset-pearl",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_model, disc_model = globals()[\"get_{}_models\".format(\n",
    "    model_dimension\n",
    ")](isize, nz, nc, ngf, extra_layers)\n",
    "gen_model.summary()\n",
    "disc_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominican-stamp",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_model.outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optical-encoding",
   "metadata": {},
   "source": [
    "### Optimizers creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proud-tissue",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_opt = tf.keras.optimizers.Adam(learning_rate=lr, beta_1=beta_1, beta_2=beta_2)\n",
    "disc_opt = tf.keras.optimizers.Adam(learning_rate=lr, beta_1=beta_1, beta_2=beta_2)\n",
    "gen_opt, disc_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binding-memorial",
   "metadata": {},
   "source": [
    "### Train and Inference steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "international-entry",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load '../models/ganomaly/utils/steps.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "characteristic-period",
   "metadata": {},
   "source": [
    "## Training process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "potential-somerset",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "potential-employer",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_data = normal_data.cache().shuffle(88, reshuffle_each_iteration=True)\n",
    "\n",
    "abnormal_data = abnormal_data.cache().shuffle(88, reshuffle_each_iteration=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increased-essex",
   "metadata": {},
   "source": [
    "### Metrics creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diagnostic-milwaukee",
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = get_true_positives()\n",
    "TN = get_true_negatives()\n",
    "FP = get_false_positives()\n",
    "FN = get_false_negatives()\n",
    "AUC = get_AUC()\n",
    "\n",
    "train_metrics_csv = open(os.path.join(metric_save_path,\"train.csv\"), \"w+\")\n",
    "train_metrics_csv.write(\"epoch, gen_error, disc_error, accuracy, precision, recall, specificity, f1_score, auc\\n\")\n",
    "\n",
    "test_metrics_csv = open(os.path.join(metric_save_path,\"test.csv\"), \"w+\")\n",
    "test_metrics_csv.write(\"epoch, accuracy, precision, recall, specificity, f1_score, auc\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embedded-frank",
   "metadata": {},
   "source": [
    "### Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qualified-roots",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Falta salvar las imagenes falsas que se van creando\n",
    "# Falta salvar los errores de los vectores apra el analisis estadistico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "variable-curtis",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    # Data partition for train and test\n",
    "    partition_point = 66\n",
    "    train_data = normal_data.enumerate().filter(\n",
    "        lambda i, xy: tf.math.less(i, partition_point)\n",
    "    ).map(undo_enumerate, N_CPUS).batch(batch_size).prefetch(-1)\n",
    "    test_data = normal_data.enumerate().filter(\n",
    "        lambda i, xy: tf.math.greater_equal(i, partition_point)\n",
    "    ).map(undo_enumerate, N_CPUS).concatenate(abnormal_data).batch(batch_size).prefetch(-1)\n",
    "    \n",
    "    for step, xy in enumerate(train_data):\n",
    "        err_g, err_d, fake_images, latent_i, latent_o, feat_real, feat_fake = train_step(xy[0])\n",
    "        \n",
    "        if err_d < 1e-5:\n",
    "            reinit_model(disc_model)\n",
    "            \n",
    "        anomaly_scores = tf.math.reduce_mean(tf.math.pow(tf.squeeze(latent_i-latent_o), 2), axis=1)\n",
    "        anomaly_scores = (anomaly_scores - tf.reduce_min(anomaly_scores)) / (\n",
    "            tf.reduce_max(anomaly_scores) - tf.reduce_min(anomaly_scores)\n",
    "        )\n",
    "            \n",
    "        TP.update_state(xy[1], anomaly_scores)\n",
    "        TN.update_state(xy[1], anomaly_scores)\n",
    "        FP.update_state(xy[1], anomaly_scores)\n",
    "        FN.update_state(xy[1], anomaly_scores)\n",
    "        AUC.update_state(xy[1], anomaly_scores)\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        print(\"\"\"\n",
    "            Epoch: {i} - Train Step: {j}\n",
    "            Generator error: {loss_g}\n",
    "            Discriminator error: {loss_d}\n",
    "            Accuracy: {acc}\n",
    "            Precision: {pre}\n",
    "            Recall: {rec}\n",
    "            Specificity: {spe}\n",
    "            F1_Score: {f1}\n",
    "            AUC: {auc}\n",
    "            \"\"\".format(\n",
    "                i = epoch + 1,\n",
    "                j = step + 1,\n",
    "                loss_g = err_g,\n",
    "                loss_d = err_d,\n",
    "                acc = accuracy(TP.result().numpy(), TN.result().numpy(), FP.result().numpy(), FN.result().numpy()),\n",
    "                pre = precision(TP.result().numpy(), FP.result().numpy()),\n",
    "                rec = recall(TP.result().numpy(), FN.result().numpy()),\n",
    "                spe = specificity(TN.result().numpy(), FP.result().numpy()),\n",
    "                f1 = f1_score(TP.result().numpy(), FP.result().numpy(), FN.result().numpy()),\n",
    "                auc = AUC.result().numpy()\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    # Save train metrics\n",
    "    train_metrics_csv.write(\"{e}, {loss_g}, {loss_d}, {acc}, {pre}, {rec}, {spe}, {f1}, {auc}\\n\".format(\n",
    "        e = epoch,\n",
    "        loss_g = err_g,\n",
    "        loss_d = err_d,\n",
    "        acc = accuracy(TP.result().numpy(), TN.result().numpy(), FP.result().numpy(), FN.result().numpy()),\n",
    "        pre = precision(TP.result().numpy(), FP.result().numpy()),\n",
    "        rec = recall(TP.result().numpy(), FN.result().numpy()),\n",
    "        spe = specificity(TN.result().numpy(), FP.result().numpy()),\n",
    "        f1 = f1_score(TP.result().numpy(), FP.result().numpy(), FN.result().numpy()),\n",
    "        auc = AUC.result().numpy()\n",
    "    ))\n",
    "    TP.reset_states()\n",
    "    TN.reset_states()\n",
    "    FP.reset_states()\n",
    "    FN.reset_states()\n",
    "    AUC.reset_states()\n",
    "    \n",
    "    for step, xy in enumerate(test_data):\n",
    "        fake_images, latent_i, latent_o, feat_real, feat_fake = test_step(xy[0])\n",
    "        \n",
    "        anomaly_scores = tf.math.reduce_mean(tf.math.pow(tf.squeeze(latent_i-latent_o), 2), axis=1)\n",
    "        anomaly_scores = (anomaly_scores - tf.reduce_min(anomaly_scores)) / (\n",
    "            tf.reduce_max(anomaly_scores) - tf.reduce_min(anomaly_scores)\n",
    "        )\n",
    "            \n",
    "        TP.update_state(xy[1], anomaly_scores)\n",
    "        TN.update_state(xy[1], anomaly_scores)\n",
    "        FP.update_state(xy[1], anomaly_scores)\n",
    "        FN.update_state(xy[1], anomaly_scores)\n",
    "        AUC.update_state(xy[1], anomaly_scores)\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        print(\"\"\"\n",
    "            Epoch: {i} - Test Step: {j}\n",
    "            Accuracy: {acc}\n",
    "            Precision: {pre}\n",
    "            Recall: {rec}\n",
    "            Specificity: {spe}\n",
    "            F1_Score: {f1}\n",
    "            AUC: {auc}\n",
    "            \"\"\".format(\n",
    "                i = epoch + 1,\n",
    "                j = step + 1,\n",
    "                acc = accuracy(TP.result().numpy(), TN.result().numpy(), FP.result().numpy(), FN.result().numpy()),\n",
    "                pre = precision(TP.result().numpy(), FP.result().numpy()),\n",
    "                rec = recall(TP.result().numpy(), FN.result().numpy()),\n",
    "                spe = specificity(TN.result().numpy(), FP.result().numpy()),\n",
    "                f1 = f1_score(TP.result().numpy(), FP.result().numpy(), FN.result().numpy()),\n",
    "                auc = AUC.result().numpy()\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    # Save test metrics\n",
    "    test_metrics_csv.write(\"{e}, {acc}, {pre}, {rec}, {spe}, {f1}, {auc}\\n\".format(\n",
    "        e = epoch,\n",
    "        acc = accuracy(TP.result().numpy(), TN.result().numpy(), FP.result().numpy(), FN.result().numpy()),\n",
    "        pre = precision(TP.result().numpy(), FP.result().numpy()),\n",
    "        rec = recall(TP.result().numpy(), FN.result().numpy()),\n",
    "        spe = specificity(TN.result().numpy(), FP.result().numpy()),\n",
    "        f1 = f1_score(TP.result().numpy(), FP.result().numpy(), FN.result().numpy()),\n",
    "        auc = AUC.result().numpy()\n",
    "    ))\n",
    "    TP.reset_states()\n",
    "    TN.reset_states()\n",
    "    FP.reset_states()\n",
    "    FN.reset_states()\n",
    "    AUC.reset_states()\n",
    "    \n",
    "train_metrics_csv.close()\n",
    "test_metrics_csv.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protected-service",
   "metadata": {},
   "source": [
    "### Save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunset-percentage",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_model.save(os.path.join(experiment_path,\"gen_model.h5\"), include_optimizer=False, save_format='h5')\n",
    "disc_model.save(os.path.join(experiment_path,\"disc_model.h5\"), include_optimizer=False, save_format='h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "public-surfing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, IntSlider\n",
    "\n",
    "x = min_max_scaler(fake_images[1], 0, 0, 1)[0].numpy()\n",
    "\n",
    "interact(lambda frame: plt.imshow(x[frame-1]), frame=IntSlider(min=1, max=x.shape[0], step=1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electric-labor",
   "metadata": {},
   "outputs": [],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dynamic-rates",
   "metadata": {},
   "outputs": [],
   "source": [
    "var = min_max_scaler(fake_images[0], 0, 0, 1)[0].numpy()\n",
    "tf.reduce_max(var), tf.reduce_min(var)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
