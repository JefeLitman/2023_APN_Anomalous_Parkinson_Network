{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "endangered-assets",
   "metadata": {},
   "source": [
    "# GANomaly Notebook for Multiple Experiment Results to save in Excels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embedded-billy",
   "metadata": {},
   "source": [
    "## Initial Configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protected-meter",
   "metadata": {},
   "source": [
    "### Libraries import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ignored-motorcycle",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from scipy import stats\n",
    "\n",
    "sys.path.append(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bored-portuguese",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.metrics import accuracy, precision, recall, specificity, f1_score\n",
    "from utils.metrics import dagostinoPearson_test, andersonDarling_test, shapiroWilks_test, chiSquare_test, fOneWay_test\n",
    "from utils.metrics import brownForsythe_test, levene_test, bartlett_test\n",
    "from utils.metrics import mannWhitney_test, kruskalWallis_test, kolmogorovSmirnov_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5694e42d",
   "metadata": {},
   "source": [
    "### Functions definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8c9648",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_curve(y_true, y_pred, num_thresholds=200):\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    thresholds = np.linspace(np.min(y_pred), np.max(y_pred), num_thresholds)\n",
    "    for t in thresholds:\n",
    "        tp = np.count_nonzero(np.logical_and(y_true, (y_pred > t)))\n",
    "        fp = np.count_nonzero(np.logical_and(np.logical_not(y_true), (y_pred > t)))\n",
    "        fn = np.count_nonzero(np.logical_and(y_true, (y_pred <= t)))\n",
    "        if tp+fp == 0:\n",
    "            precisions.append(0)\n",
    "        else:\n",
    "            precisions.append(precision(tp, fp))\n",
    "        if tp + fn == 0:\n",
    "            recalls.append(0)\n",
    "        else:\n",
    "            recalls.append(recall(tp, fn))\n",
    "    return np.r_[precisions], np.r_[recalls], thresholds\n",
    "\n",
    "def format_index(index, max_digits = 4):\n",
    "    \"\"\"This function format the index integer into a string with the maximun quantity of digits geivn.\n",
    "    Args:\n",
    "        index (Int): Integer to be formatted.\n",
    "        max_digits (Int): How many digits must the number contain, e.g: if 4 then the range is from 0000 to 9999.\n",
    "    \"\"\"\n",
    "    value = str(index)\n",
    "    while len(value) < max_digits:\n",
    "        value = '0' + value\n",
    "    return value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eec12b3",
   "metadata": {},
   "source": [
    "### Experiments selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5551c490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiments_ids = [\n",
    "#     \"0028\", \"0032\", \"0034\", \"0035\", # 3D B32 BN Control\n",
    "#     \"0038\", \"0042\", \"0047\", \"0052\", # 2D B32 BN Control\n",
    "#     \"0029\", \"0033\", \"0036\", \"0037\", # 3D B16 BN Control\n",
    "#     \"0039\", \"0043\", \"0050\", \"0055\", # 2D B16 BN Control\n",
    "#     \"0044\", \"0046\", \"0048\", \"0051\", # 3D B16 BN Parkinson\n",
    "#     \"0063\", \"0067\", \"0069\", \"0070\", # 2D B16 BN Parkinson\n",
    "#     \"0040\", \"0041\", \"0045\", \"0049\", # 3D B16 RGB Control\n",
    "#     \"0053\", \"0058\", \"0061\", \"0066\", # 2D B16 RGB Control\n",
    "#     \"0059\", \"0062\", \"0065\", \"0068\", # 3D B16 RGB Parkinson\n",
    "#     \"0054\", \"0057\", \"0060\", \"0064\", # 2D B16 RGB Parkinson\n",
    "# ]\n",
    "experiments_ids = [format_index(i) for i in range(1, 28)]\n",
    "save_path_2_excels = \"/home/jefelitman/Results_Reports/\"\n",
    "\n",
    "exps = {}\n",
    "root_path = \"/home/jefelitman/Saved_Models/Anomaly_parkinson/\"\n",
    "for exp_id in experiments_ids:\n",
    "    for i in sorted(os.listdir(root_path)):\n",
    "        if exp_id in i:\n",
    "            exp_path = os.path.join(root_path, i)\n",
    "            exps[exp_id] = {\n",
    "                \"path\": exp_path\n",
    "            }\n",
    "exps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308e4736",
   "metadata": {},
   "source": [
    "## Standar metrics excel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8092b9",
   "metadata": {},
   "source": [
    "### Errors loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "configured-tyler",
   "metadata": {},
   "outputs": [],
   "source": [
    "for exp_id in experiments_ids:\n",
    "    base_path = os.path.join(exps[exp_id][\"path\"], \"outputs/errors/\")\n",
    "    for t in [\"encoder\", \"contextual\", \"adversarial\"]:\n",
    "            for c in [\"normal\", \"abnormal\"]:\n",
    "                exps[exp_id][\"all_{}_{}\".format(t, c)] = np.r_[[]]\n",
    "            \n",
    "    for t in [\"encoder\", \"contextual\", \"adversarial\"]:\n",
    "        for m in [\"train\", \"test\"]:\n",
    "            if m == \"train\":\n",
    "                if os.path.isfile(os.path.join(base_path, t, m, \"normal.npy\")):\n",
    "                    classes = [\"normal\"] \n",
    "                else:\n",
    "                    classes = [\"abnormal\"]\n",
    "            else:\n",
    "                classes = [\"normal\", \"abnormal\"]\n",
    "\n",
    "            for c in classes:\n",
    "                all_data = \"all_{}_{}\".format(t, c)\n",
    "                errors = np.load(os.path.join(base_path, t, m, c + \".npy\"))\n",
    "                exps[exp_id][\"{}_{}_{}\".format(m, t, c)] = errors\n",
    "                exps[exp_id][all_data] = np.concatenate([exps[exp_id][all_data], errors])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10791fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for exp_id in experiments_ids:\n",
    "    base_path = os.path.join(exps[exp_id][\"path\"], \"outputs/latent_vectors/input_generator\")\n",
    "    for t in [\"encoder\", \"contextual\", \"adversarial\"]:\n",
    "        for m in [\"train\", \"test\"]:\n",
    "            if m == \"train\":\n",
    "                if \"train_encoder_normal\" in exps[exp_id].keys():\n",
    "                    classes = [\"normal\"] \n",
    "                else:\n",
    "                    classes = [\"abnormal\"]\n",
    "            else:\n",
    "                classes = [\"normal\", \"abnormal\"]\n",
    "            for c in classes:\n",
    "                patients_ids_positions = [\n",
    "                    int(i.split(\"-\")[1].split(\".\")[0]) for i in sorted(\n",
    "                        os.listdir(os.path.join(base_path, m, c))\n",
    "                    )\n",
    "                ]\n",
    "                data = \"{}_{}_{}\".format(m, t, c)\n",
    "                key = \"{}_{}\".format(data, \"patients\")\n",
    "                exps[exp_id][key] = {}\n",
    "                \n",
    "                for p_id in np.unique(patients_ids_positions):\n",
    "                    exps[exp_id][key][p_id] = []\n",
    "                \n",
    "                for i, p_id in enumerate(patients_ids_positions):\n",
    "                    exps[exp_id][key][p_id].append(exps[exp_id][data][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norwegian-decline",
   "metadata": {},
   "source": [
    "### Building excel with standard metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optimum-genetics",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_table = []\n",
    "data_columns = [\"Exp ID\", \"L_gen\", \"L_disc\", \"Acc (t=0.5)\", \"Pre_orig\", \n",
    "                \"Rec_orig\", \"Spe_orig\", \"F1_orig\", \"AUC\", \"Threshold\", \n",
    "                \"Acc_thre\", \"Pre_thre\", \"Rec_thre\", \"Spe_thre\", \"F1_thre\",\n",
    "                \"Homo Level\"\n",
    "               ]\n",
    "for exp_id in experiments_ids:\n",
    "    experiment_folder = exps[exp_id][\"path\"]\n",
    "    \n",
    "    train_metrics = pd.read_csv(os.path.join(experiment_folder, \"metrics/train.csv\"))\n",
    "    test_metrics = pd.read_csv(os.path.join(experiment_folder, \"metrics/test.csv\"))\n",
    "\n",
    "    group = \"encoder\"\n",
    "    data = \"test_{}_\".format(group)\n",
    "    errors = [\"normal\", \"abnormal\"]\n",
    "    path = os.path.join(experiment_folder, \"outputs/graphics/quantitative/\")\n",
    "    y_true = np.concatenate([[i]*exps[exp_id][data + j].shape[0] for i,j in enumerate(errors)]) \n",
    "    y_pred = np.concatenate([exps[exp_id][data+i] for i in errors])\n",
    "    \n",
    "    if \"train_{}_normal\".format(group) in exps[exp_id].keys():\n",
    "        y_pred = (y_pred - np.min(y_pred)) / (np.max(y_pred) - np.min(y_pred))\n",
    "        test_class = \"normal\"\n",
    "    else:\n",
    "        y_pred = 1 - ((y_pred - np.min(y_pred)) / (np.max(y_pred) - np.min(y_pred)))\n",
    "        test_class = \"abnormal\"\n",
    "    \n",
    "    precisions, recalls, thresholds = precision_recall_curve(y_true, y_pred)\n",
    "    deltas_pre_4_rec = np.abs(precisions - recalls)\n",
    "    threshold = thresholds[np.argmin(deltas_pre_4_rec[deltas_pre_4_rec != 0])]\n",
    "\n",
    "    TP = tf.keras.metrics.TruePositives(threshold)\n",
    "    TN = tf.keras.metrics.TrueNegatives(threshold)\n",
    "    FP = tf.keras.metrics.FalsePositives(threshold)\n",
    "    FN = tf.keras.metrics.FalseNegatives(threshold)\n",
    "    AUC = tf.keras.metrics.AUC()\n",
    "\n",
    "    TP.update_state(y_true, y_pred)\n",
    "    TN.update_state(y_true, y_pred)\n",
    "    FP.update_state(y_true, y_pred)\n",
    "    FN.update_state(y_true, y_pred)\n",
    "    AUC.update_state(y_true, y_pred)\n",
    "    \n",
    "    homo_key = \"{}{}_patients\".format(data, test_class)\n",
    "    homo_metric = 0\n",
    "    for p_id in exps[exp_id][homo_key]:\n",
    "        homo_4_pat = int(brownForsythe_test(\n",
    "            sorted(exps[exp_id][homo_key][p_id]), sorted(exps[exp_id][\"train_{}_{}\".format(group, test_class)])\n",
    "        )) + int(levene_test(\n",
    "            sorted(exps[exp_id][homo_key][p_id]), sorted(exps[exp_id][\"train_{}_{}\".format(group, test_class)])\n",
    "        )) + int(bartlett_test(\n",
    "            sorted(exps[exp_id][homo_key][p_id]), sorted(exps[exp_id][\"train_{}_{}\".format(group, test_class)])\n",
    "        ))\n",
    "        homo_metric += homo_4_pat/3\n",
    "    homo_metric /= len(exps[exp_id][homo_key])\n",
    "    \n",
    "    data_table.append([\n",
    "        exp_id,\n",
    "        train_metrics.loc[train_metrics.shape[0] - 1 ,\"gen_error\"],\n",
    "        train_metrics.loc[train_metrics.shape[0] - 1 ,\"disc_error\"],\n",
    "        test_metrics.loc[test_metrics.shape[0] - 1 ,\"accuracy\"],\n",
    "        test_metrics.loc[test_metrics.shape[0] - 1 ,\"precision\"],\n",
    "        test_metrics.loc[test_metrics.shape[0] - 1 ,\"recall\"],\n",
    "        test_metrics.loc[test_metrics.shape[0] - 1 ,\"specificity\"],\n",
    "        test_metrics.loc[test_metrics.shape[0] - 1 ,\"f1_score\"],\n",
    "        AUC.result().numpy(),\n",
    "        threshold,\n",
    "        np.max(accuracy(TP.result().numpy(), TN.result().numpy(), FP.result().numpy(), FN.result().numpy())),\n",
    "        np.max(precision(TP.result().numpy(), FP.result().numpy())),\n",
    "        np.max(recall(TP.result().numpy(), FN.result().numpy())),\n",
    "        np.max(specificity(TN.result().numpy(), FP.result().numpy())),\n",
    "        np.max(f1_score(TP.result().numpy(), FP.result().numpy(), FN.result().numpy())),\n",
    "        homo_metric\n",
    "    ])\n",
    "\n",
    "pd.DataFrame(data_table, columns=data_columns).to_excel(os.path.join(save_path_2_excels, \"standard_metrics.xlsx\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "olympic-recipient",
   "metadata": {},
   "source": [
    "## Qualitative metrics excel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dynamic-remains",
   "metadata": {},
   "source": [
    "### Errors exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disciplinary-label",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_table = []\n",
    "data_columns = [\"Exp ID\", \"Min\", \"Max\", \"Mean\", \"Std\", \"Ske\", \"Kur\", \"CDF (x > 0)\"]\n",
    "\n",
    "for g in [\"train\", \"test\", \"all\"]:\n",
    "        if g == \"train\":\n",
    "            classes = [\"check\"]\n",
    "        else:\n",
    "            classes = [\"normal\", \"abnormal\"]\n",
    "        for cl in classes:\n",
    "            for t in [\"encoder\", \"contextual\", \"adversarial\"]:\n",
    "                \n",
    "                if cl == \"check\":\n",
    "                    data_table.append([\"{} {}\".format(g, t)] + [None]*5)\n",
    "                else:\n",
    "                    data_table.append([\"{} {} {}\".format(g, t, cl)] + [None]*5)\n",
    "                    \n",
    "                for exp_id in experiments_ids:\n",
    "                    if cl == \"check\":\n",
    "                        if \"train_{}_normal\".format(t) in exps[exp_id].keys():\n",
    "                            c = \"normal\"\n",
    "                        else:\n",
    "                            c = \"abnormal\"\n",
    "                    else:\n",
    "                        c = cl\n",
    "                    \n",
    "                    data = exps[exp_id][\"{}_{}_{}\".format(g, t, c)]\n",
    "                    m = np.mean(data)\n",
    "                    s = np.std(data)\n",
    "                \n",
    "                    data_table.append([\n",
    "                        exp_id,\n",
    "                        np.min(data),\n",
    "                        np.max(data),\n",
    "                        m,\n",
    "                        s,\n",
    "                        stats.skew(data),\n",
    "                        stats.kurtosis(data),\n",
    "                        1 - stats.norm(m, s).cdf(0)\n",
    "                    ])\n",
    "pd.DataFrame(data_table, columns=data_columns).to_excel(\n",
    "    os.path.join(save_path_2_excels, \"qualitative_metrics.xlsx\"), \n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opponent-status",
   "metadata": {},
   "source": [
    "### Normality tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuing-installation",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_table = []\n",
    "data_columns = [\"Exp ID\", \"Brown\", \"Levene\", \"Barlett\", \"DAP\", \"AD\", \"SW\", \"Chi^2\", \"F test\"]\n",
    "\n",
    "for g in [\"train\", \"test\", \"all\"]:\n",
    "    if g == \"train\":\n",
    "        classes = [\"check\"]\n",
    "    else:\n",
    "        classes = [\"normal\", \"abnormal\"]\n",
    "    for cl in classes:\n",
    "        for t in [\"encoder\", \"contextual\", \"adversarial\"]:\n",
    "\n",
    "            if cl == \"check\":\n",
    "                data_table.append([\"{} {}\".format(g, t)] + [None]*8)\n",
    "            else:\n",
    "                data_table.append([\"{} {} {}\".format(g, t, cl)] + [None]*8)\n",
    "\n",
    "            for exp_id in experiments_ids:\n",
    "                if cl == \"check\":\n",
    "                    if \"train_{}_normal\".format(t) in exps[exp_id].keys():\n",
    "                        c = \"normal\"\n",
    "                    else:\n",
    "                        c = \"abnormal\"\n",
    "                else:\n",
    "                    c = cl\n",
    "\n",
    "                data = sorted(exps[exp_id][\"{}_{}_{}\".format(g, t, c)])\n",
    "                norm_dist = sorted(stats.norm.rvs(loc=np.mean(data), scale=np.std(data), size=len(data), random_state=8128))\n",
    "                chi_test = chiSquare_test(data, norm_dist)\n",
    "                data_table.append([\n",
    "                    exp_id,\n",
    "                    int(brownForsythe_test(data, norm_dist)),\n",
    "                    int(levene_test(data, norm_dist)),\n",
    "                    int(bartlett_test(data, norm_dist)),\n",
    "                    int(dagostinoPearson_test(data)),\n",
    "                    int(andersonDarling_test(data)),\n",
    "                    int(shapiroWilks_test(data)),\n",
    "                    \"{} ({})\".format(int(chi_test[0]), round(chi_test[1], 5)),\n",
    "                    int(fOneWay_test(data, norm_dist))\n",
    "                ])\n",
    "pd.DataFrame(data_table, columns=data_columns).to_excel(\n",
    "    os.path.join(save_path_2_excels, \"normality_tests.xlsx\"), \n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41352f71",
   "metadata": {},
   "source": [
    "### Grouping tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e810c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_table = []\n",
    "data_columns = [\"Exp ID\", \"Brown\", \"Levene\", \"Barlett\", \"MW\", \"KW\", \"KS\", \"F test\"]\n",
    "\n",
    "for t in [\"encoder\", \"contextual\", \"adversarial\"]:\n",
    "    for g1, g2 in [(\"train\", \"test\"), (\"test\", \"all\"), (\"train\", \"all\")]:\n",
    "        data_table.append([\"{} {} vs {}\".format(t, g1, g2)] + [None]*7)\n",
    "        for exp_id in experiments_ids:\n",
    "            if \"train_{}_normal\".format(t) in exps[exp_id].keys():\n",
    "                c = \"normal\"\n",
    "            else:\n",
    "                c = \"abnormal\"\n",
    "                \n",
    "            data1 = sorted(exps[exp_id][\"{}_{}_{}\".format(g1, t, c)])\n",
    "            data2 = sorted(exps[exp_id][\"{}_{}_{}\".format(g2, t, c)])\n",
    "            \n",
    "            data_table.append([\n",
    "                exp_id,\n",
    "                int(brownForsythe_test(data1, data2)),\n",
    "                int(levene_test(data1, data2)),\n",
    "                int(bartlett_test(data1, data2)),\n",
    "                int(mannWhitney_test(data1, data2)),\n",
    "                int(kruskalWallis_test(data1, data2)),\n",
    "                int(kolmogorovSmirnov_test(data1, data2)),\n",
    "                int(fOneWay_test(data1, data2))\n",
    "            ])\n",
    "pd.DataFrame(data_table, columns=data_columns).to_excel(\n",
    "    os.path.join(save_path_2_excels, \"grouping_tests.xlsx\"), \n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5991a650",
   "metadata": {},
   "source": [
    "### Classing tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a34b1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_table = []\n",
    "data_columns = [\"Exp ID\", \"Brown\", \"Levene\", \"Barlett\", \"Chi^2 N->A\", \"Chi^2 A->N\", \"MW\", \"KW\", \"KS\", \"F test\"]\n",
    "\n",
    "for g in [\"test\", \"all\"]:\n",
    "    for t in [\"encoder\", \"contextual\", \"adversarial\"]:\n",
    "        data_table.append([\"{} {}\".format(g, t)] + [None]*9)\n",
    "        for exp_id in experiments_ids:\n",
    "            data1 = sorted(exps[exp_id][\"{}_{}_normal\".format(g, t)])\n",
    "            data2 = sorted(exps[exp_id][\"{}_{}_abnormal\".format(g, t)])\n",
    "            \n",
    "            row = [\n",
    "                exp_id,\n",
    "                int(brownForsythe_test(data1, data2)),\n",
    "                int(levene_test(data1, data2)),\n",
    "                int(bartlett_test(data1, data2)),\n",
    "            ]\n",
    "            if g == \"test\":\n",
    "                row += [\n",
    "                    int(mannWhitney_test(data1, data2)),\n",
    "                    int(kruskalWallis_test(data1, data2)),\n",
    "                    int(kolmogorovSmirnov_test(data1, data2)),\n",
    "                    int(fOneWay_test(data1, data2)),\n",
    "                    None,\n",
    "                    None\n",
    "                ]\n",
    "            else:\n",
    "                chi_1_test = chiSquare_test(data1, data2)\n",
    "                chi_2_test = chiSquare_test(data2, data1)\n",
    "                row += [\n",
    "                    \"{} ({})\".format(int(chi_1_test[0]), round(chi_1_test[1], 5)),\n",
    "                    \"{} ({})\".format(int(chi_2_test[0]), round(chi_2_test[1], 5)),\n",
    "                    int(mannWhitney_test(data1, data2)),\n",
    "                    int(kruskalWallis_test(data1, data2)),\n",
    "                    int(kolmogorovSmirnov_test(data1, data2)),\n",
    "                    int(fOneWay_test(data1, data2))\n",
    "                ]\n",
    "            \n",
    "            data_table.append(row)\n",
    "        \n",
    "pd.DataFrame(data_table, columns=data_columns).to_excel(\n",
    "    os.path.join(save_path_2_excels, \"classing_tests.xlsx\"), \n",
    "    index=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
