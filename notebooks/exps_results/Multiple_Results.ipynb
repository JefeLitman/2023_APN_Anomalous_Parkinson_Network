{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "endangered-assets",
   "metadata": {},
   "source": [
    "# Notebook for Multiple Experiment Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protected-meter",
   "metadata": {},
   "source": [
    "### Libraries import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ignored-motorcycle",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "sys.path.append(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bored-portuguese",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.common import format_index, repeat_vector_to_size\n",
    "from utils.metrics import precision_recall_curve, tpr_fpr_curve\n",
    "from utils.metrics import chiSquare_test, brownForsythe_test, levene_test\n",
    "from utils.metrics import accuracy, precision, recall, specificity, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eec12b3",
   "metadata": {},
   "source": [
    "### Experiments selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5551c490",
   "metadata": {},
   "outputs": [],
   "source": [
    "major_exps_ids = [format_index(i) for i in [12, 10, 8, 13, 11, 9]]\n",
    "minor_exps_ids = [format_index(i) for i in range(1, 22)]\n",
    "save_path_2_excels = \"../../results/Results_Reports/\"\n",
    "\n",
    "exps = {}\n",
    "seed = 8128\n",
    "root_path = \"../../results/Ganomaly_3D/\"\n",
    "\n",
    "for i in major_exps_ids:\n",
    "    for exp_id in sorted(os.listdir(root_path)):\n",
    "        if i in exp_id:\n",
    "            sub_path = os.path.join(root_path, exp_id)\n",
    "            for j in minor_exps_ids:\n",
    "                for subexp_id in sorted(os.listdir(sub_path)):\n",
    "                    if j in subexp_id:\n",
    "                        final_id = \"G3D_{}_{}\".format(i, int(j))\n",
    "                        exp_path = os.path.join(sub_path, subexp_id)\n",
    "                        exps[final_id] = {\n",
    "                            \"path\": exp_path\n",
    "                        }\n",
    "\n",
    "exps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01ba969",
   "metadata": {},
   "source": [
    "### Errors loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf295a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "for exp_id in exps:\n",
    "    base_path = os.path.join(exps[exp_id][\"path\"], \"outputs/errors/\")\n",
    "    for t in [\"encoder\", \"contextual\", \"adversarial\"]:\n",
    "        for c in [\"normal\", \"abnormal\"]:\n",
    "            exps[exp_id][\"all_{}_{}\".format(t, c)] = np.r_[[]]\n",
    "\n",
    "    for t in [\"encoder\", \"contextual\", \"adversarial\"]:\n",
    "        for m in [\"train\", \"val\", \"test\"]:\n",
    "            if m == \"train\":\n",
    "                if os.path.isfile(os.path.join(base_path, t, m, \"normal.npy\")):\n",
    "                    classes = [\"normal\"] \n",
    "                else:\n",
    "                    classes = [\"abnormal\"]\n",
    "            else:\n",
    "                classes = [\"normal\", \"abnormal\"]\n",
    "\n",
    "            for c in classes:\n",
    "                all_data = \"all_{}_{}\".format(t, c)\n",
    "                errors = np.load(os.path.join(base_path, t, m, c + \".npy\"))\n",
    "                exps[exp_id][\"{}_{}_{}\".format(m, t, c)] = errors\n",
    "                if m != \"train\":\n",
    "                    exps[exp_id][all_data] = np.concatenate([exps[exp_id][all_data], errors])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ffd415",
   "metadata": {},
   "source": [
    "### Errors by patients loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f2b8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "for exp_id in exps:\n",
    "    base_path = os.path.join(exps[exp_id][\"path\"], \"outputs/latent_vectors/input_generator\")\n",
    "    for t in [\"encoder\", \"contextual\", \"adversarial\"]:\n",
    "        for c in [\"normal\", \"abnormal\"]:\n",
    "            exps[exp_id][\"all_{}_{}_patients\".format(t, c)] = {}\n",
    "\n",
    "    for t in [\"encoder\", \"contextual\", \"adversarial\"]:\n",
    "        for m in [\"train\", \"val\", \"test\"]:\n",
    "            if m == \"train\":\n",
    "                if \"train_encoder_normal\" in exps[exp_id].keys():\n",
    "                    classes = [\"normal\"] \n",
    "                else:\n",
    "                    classes = [\"abnormal\"]\n",
    "            else:\n",
    "                classes = [\"normal\", \"abnormal\"]\n",
    "            for c in classes:\n",
    "                patients_ids_positions = [\n",
    "                    int(i.split(\"_\")[1].split(\"-\")[1].split(\".\")[0]) for i in sorted(\n",
    "                        os.listdir(os.path.join(base_path, m, c))\n",
    "                    )\n",
    "                ]\n",
    "                all_data = \"all_{}_{}_patients\".format(t, c)\n",
    "                data = \"{}_{}_{}\".format(m, t, c)\n",
    "                key = \"{}_{}\".format(data, \"patients\")\n",
    "                exps[exp_id][key] = {}\n",
    "\n",
    "                for p_id in np.unique(patients_ids_positions):\n",
    "                    exps[exp_id][key][p_id] = []\n",
    "                    exps[exp_id][all_data][p_id] = []\n",
    "\n",
    "                for i, p_id in enumerate(patients_ids_positions):\n",
    "                    exps[exp_id][key][p_id].append(exps[exp_id][data][i])\n",
    "                    if m != \"train\":\n",
    "                        exps[exp_id][all_data][p_id].append(exps[exp_id][data][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9d53c3",
   "metadata": {},
   "source": [
    "### Quantitative metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86b88c5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_table = []\n",
    "data_columns = [\"Exp ID\", \"Group\", \"Partition\", \"AUC\", \"Threshold\", \"Acc\", \"Pre\", \"Rec\", \"Spe\", \"F1\", \"Homo\", \"Class\"]\n",
    "\n",
    "errors = [\"normal\", \"abnormal\"]\n",
    "\n",
    "for t in [\"encoder\", \"contextual\", \"adversarial\"]:\n",
    "    for exp_id in exps:\n",
    "        for part in [\"val\", \"test\", \"all\"]:\n",
    "            data = \"{}_{}_\".format(part, t)\n",
    "            y_true = np.concatenate([[i]*exps[exp_id][data + j].shape[0] for i,j in enumerate(errors)]) \n",
    "            y_pred = np.concatenate([exps[exp_id][data+i] for i in errors])\n",
    "            tpr, fpr, _ = tpr_fpr_curve(y_true, y_pred)\n",
    "\n",
    "            if part == \"val\":\n",
    "                precisions, recalls, thresholds = precision_recall_curve(y_true, y_pred)\n",
    "                deltas_pre_4_rec = np.abs(precisions - recalls)\n",
    "                threshold = thresholds[np.argmin(deltas_pre_4_rec[deltas_pre_4_rec != 0])]\n",
    "\n",
    "            if \"train_{}_normal\".format(t) in exps[exp_id].keys():\n",
    "                y_pred = (y_pred > threshold).astype(np.int64)\n",
    "                trained_class = \"normal\"\n",
    "            else:\n",
    "                y_pred = (y_pred < threshold).astype(np.int64)\n",
    "                trained_class = \"abnormal\"\n",
    "\n",
    "            TP = tf.keras.metrics.TruePositives()\n",
    "            TN = tf.keras.metrics.TrueNegatives()\n",
    "            FP = tf.keras.metrics.FalsePositives()\n",
    "            FN = tf.keras.metrics.FalseNegatives()\n",
    "\n",
    "            TP.update_state(y_true, y_pred)\n",
    "            TN.update_state(y_true, y_pred)\n",
    "            FP.update_state(y_true, y_pred)\n",
    "            FN.update_state(y_true, y_pred)\n",
    "            \n",
    "            # Homocedasticity metric\n",
    "            homo_level = []\n",
    "            class_level = []\n",
    "            for c in [\"normal\", \"abnormal\"]:\n",
    "                if c == trained_class:\n",
    "                    groups = [\n",
    "                        (\"train\", \"val\"), (\"train\", \"test\"), (\"val\", \"test\")\n",
    "                    ]\n",
    "                    prefix = 0\n",
    "                else:\n",
    "                    groups = [\n",
    "                        (\"train\", \"val\"), (\"train\", \"test\"), \n",
    "                        (\"val\", \"val\"), (\"val\", \"test\"), \n",
    "                        (\"test\", \"val\"), (\"test\", \"test\"), \n",
    "                    ]\n",
    "                    prefix = 1\n",
    "                for g1, g2 in groups:\n",
    "                    data1 = exps[exp_id][\"{}_{}_{}\".format(g1, t, trained_class)]\n",
    "                    data2 = exps[exp_id][\"{}_{}_{}\".format(g2, t, c)]\n",
    "                    if data1.shape[0] > data2.shape[0]:\n",
    "                        sub_data1 = np.r_[sorted(data1)]\n",
    "                        sub_data2 = np.r_[sorted(repeat_vector_to_size(data2, data1.shape[0], seed))]\n",
    "                    elif data1.shape[0] < data2.shape[0]:\n",
    "                        sub_data1 = np.r_[sorted(repeat_vector_to_size(data1, data2.shape[0], seed))]\n",
    "                        sub_data2 = np.r_[sorted(data2)]\n",
    "                    else:\n",
    "                        sub_data1 = np.r_[sorted(data1)]\n",
    "                        sub_data2 = np.r_[sorted(data2)]\n",
    "                    chi_test_1 = chiSquare_test(sub_data1, sub_data2)\n",
    "                    chi_test_2 = chiSquare_test(sub_data2, sub_data1)\n",
    "\n",
    "                    data1 = np.r_[sorted(data1)]\n",
    "                    data2 = np.r_[sorted(data2)]\n",
    "                    homo_level += [\n",
    "                        abs(prefix - int(brownForsythe_test(data1, data2))), \n",
    "                        abs(prefix - int(levene_test(data1, data2)))\n",
    "                    ]\n",
    "                    class_level += [\n",
    "                        abs(prefix - int(chi_test_1[0])), \n",
    "                        abs(prefix - int(chi_test_2[0]))\n",
    "                    ]\n",
    "            # Calculation of the last pair of data for non trained class\n",
    "            if trained_class == \"normal\":\n",
    "                data1 = exps[exp_id][\"{}_{}_{}\".format(\"val\", t, \"abnormal\")]\n",
    "                data2 = exps[exp_id][\"{}_{}_{}\".format(\"test\", t, \"abnormal\")]\n",
    "            else:\n",
    "                data1 = exps[exp_id][\"{}_{}_{}\".format(\"val\", t, \"normal\")]\n",
    "                data2 = exps[exp_id][\"{}_{}_{}\".format(\"test\", t, \"normal\")]\n",
    "            if data1.shape[0] > data2.shape[0]:\n",
    "                sub_data1 = np.r_[sorted(data1)]\n",
    "                sub_data2 = np.r_[sorted(repeat_vector_to_size(data2, data1.shape[0], seed))]\n",
    "            elif data1.shape[0] < data2.shape[0]:\n",
    "                sub_data1 = np.r_[sorted(repeat_vector_to_size(data1, data2.shape[0], seed))]\n",
    "                sub_data2 = np.r_[sorted(data2)]\n",
    "            else:\n",
    "                sub_data1 = np.r_[sorted(data1)]\n",
    "                sub_data2 = np.r_[sorted(data2)]\n",
    "            chi_test_1 = chiSquare_test(sub_data1, sub_data2)\n",
    "            chi_test_2 = chiSquare_test(sub_data2, sub_data1)\n",
    "            data1 = np.r_[sorted(data1)]\n",
    "            data2 = np.r_[sorted(data2)]\n",
    "            homo_level += [\n",
    "                int(brownForsythe_test(data1, data2)), \n",
    "                int(levene_test(data1, data2))\n",
    "            ]\n",
    "            class_level += [\n",
    "                int(chi_test_1[0]), \n",
    "                int(chi_test_2[0])\n",
    "            ]\n",
    "                        \n",
    "            data_table.append([\n",
    "                exp_id,\n",
    "                t,\n",
    "                part,\n",
    "                round(auc(fpr, tpr), 3),\n",
    "                round(threshold, 3),\n",
    "                round(accuracy(TP.result().numpy(), TN.result().numpy(), FP.result().numpy(), FN.result().numpy()), 3),\n",
    "                round(precision(TP.result().numpy(), FP.result().numpy()), 3),\n",
    "                round(recall(TP.result().numpy(), FN.result().numpy()), 3),\n",
    "                round(specificity(TN.result().numpy(), FP.result().numpy()), 3),\n",
    "                round(f1_score(TP.result().numpy(), FP.result().numpy(), FN.result().numpy()), 3),\n",
    "                round(np.mean(homo_level), 3),\n",
    "                round(np.mean(class_level), 3)\n",
    "            ])\n",
    "df = pd.DataFrame(data_table, columns=data_columns)\n",
    "\n",
    "for exp_id_1, exp_id_2, new_id in [\n",
    "    (\"G3D_0012_1\", \"G3D_0010_1\", \"G3D_Control_1\"),\n",
    "    (\"G3D_0012_11\", \"G3D_0010_2\", \"G3D_Control_1000\"),\n",
    "    (\"G3D_0012_21\", \"G3D_0010_3\", \"G3D_Control_2000\"),\n",
    "    (\"G3D_0008_1\", \"G3D_0010_6\", \"G3D_Control_5000\"),\n",
    "    (\"G3D_0008_2\", \"G3D_0010_11\", \"G3D_Control_10000\"),\n",
    "    (\"G3D_0008_3\", \"G3D_0010_16\", \"G3D_Control_15000\"),\n",
    "    (\"G3D_0008_4\", \"G3D_0010_21\", \"G3D_Control_20000\"),\n",
    "    (\"G3D_0013_1\", \"G3D_0011_1\", \"G3D_Park_1\"),\n",
    "    (\"G3D_0013_11\", \"G3D_0011_2\", \"G3D_Park_1000\"),\n",
    "    (\"G3D_0013_21\", \"G3D_0011_3\", \"G3D_Park_2000\"),\n",
    "    (\"G3D_0009_1\", \"G3D_0011_6\", \"G3D_Park_5000\"),\n",
    "    (\"G3D_0009_2\", \"G3D_0011_11\", \"G3D_Park_10000\"),\n",
    "    (\"G3D_0009_3\", \"G3D_0011_16\", \"G3D_Park_15000\"),\n",
    "    (\"G3D_0009_4\", \"G3D_0011_21\", \"G3D_Park_20000\"),\n",
    "]:\n",
    "    column_ids = np.r_[df.loc[df[\"Exp ID\"] == exp_id_1].shape[0] * [new_id]].reshape([-1, 1])\n",
    "    columns_fields = df.loc[df[\"Exp ID\"] == exp_id_1].values[:, 1:3]\n",
    "    data1 = df.loc[df[\"Exp ID\"] == exp_id_1].values[:, 3:]\n",
    "    data2 = df.loc[df[\"Exp ID\"] == exp_id_2].values[:, 3:]\n",
    "    average = (data1 + data2) / 2\n",
    "    concat_data = pd.DataFrame(np.concatenate([column_ids, columns_fields, average], axis=1), columns = df.columns)\n",
    "    df = pd.concat([df, concat_data], axis=0)\n",
    "    \n",
    "df.to_excel(os.path.join(save_path_2_excels, \"quantitative_metrics.xlsx\"), index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36eca6b8",
   "metadata": {},
   "source": [
    "### Classing Qualitative Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffc3f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_table = []\n",
    "data_columns = [\n",
    "    \"Exp ID\", \"Element\", \"Group\", \"vs Group\", \"Homo\", \"Class\", \n",
    "    \"Chi2 N -> A\", \"Delta Chi2 N -> A\", \"Chi2 A -> N\", \"Delta Chi2 A -> N\"\n",
    "]\n",
    "for t in [\"encoder\", \"contextual\", \"adversarial\"]:\n",
    "    for g1 in [\"train\", \"val\", \"test\"]:\n",
    "        for g2 in [\"val\", \"test\"]:\n",
    "            for exp_id in exps:\n",
    "                if \"train_{}_normal\".format(t) in exps[exp_id].keys():\n",
    "                    data1 = exps[exp_id][\"{}_{}_normal\".format(g1, t)]\n",
    "                    data2 = exps[exp_id][\"{}_{}_abnormal\".format(g2, t)]\n",
    "                else:\n",
    "                    data2 = exps[exp_id][\"{}_{}_abnormal\".format(g1, t)]\n",
    "                    data1 = exps[exp_id][\"{}_{}_normal\".format(g2, t)]\n",
    "                if data1.shape[0] > data2.shape[0]:\n",
    "                    sub_data1 = np.r_[sorted(data1)]\n",
    "                    sub_data2 = np.r_[sorted(repeat_vector_to_size(data2, data1.shape[0], seed))]\n",
    "                elif data1.shape[0] < data2.shape[0]:\n",
    "                    sub_data1 = np.r_[sorted(repeat_vector_to_size(data1, data2.shape[0], seed))]\n",
    "                    sub_data2 = np.r_[sorted(data2)]\n",
    "                else:\n",
    "                    sub_data1 = np.r_[sorted(data1)]\n",
    "                    sub_data2 = np.r_[sorted(data2)]\n",
    "                chi_test_1 = chiSquare_test(sub_data1, sub_data2)\n",
    "                chi_test_2 = chiSquare_test(sub_data2, sub_data1)\n",
    "                data1 = np.r_[sorted(data1)]\n",
    "                data2 = np.r_[sorted(data2)]\n",
    "                homo_level = [abs(1 - int(brownForsythe_test(data1, data2))), abs(1 - int(levene_test(data1, data2)))]\n",
    "                class_level = [abs(1 - int(chi_test_1[0])), abs(1 - int(chi_test_2[0]))]\n",
    "                data_table.append([\n",
    "                    exp_id,\n",
    "                    t,\n",
    "                    g1,\n",
    "                    g2, \n",
    "                    np.mean(homo_level),\n",
    "                    np.mean(class_level),\n",
    "                    int(chi_test_1[0]), \n",
    "                    round(chi_test_1[1], 5),\n",
    "                    int(chi_test_2[0]), \n",
    "                    round(chi_test_2[1], 5),\n",
    "                ])\n",
    "df = pd.DataFrame(data_table, columns=data_columns)\n",
    "\n",
    "for exp_id_1, exp_id_2, new_id in [\n",
    "    (\"G3D_0012_1\", \"G3D_0010_1\", \"G3D_Control_1\"),\n",
    "    (\"G3D_0012_11\", \"G3D_0010_2\", \"G3D_Control_1000\"),\n",
    "    (\"G3D_0012_21\", \"G3D_0010_3\", \"G3D_Control_2000\"),\n",
    "    (\"G3D_0008_1\", \"G3D_0010_6\", \"G3D_Control_5000\"),\n",
    "    (\"G3D_0008_2\", \"G3D_0010_11\", \"G3D_Control_10000\"),\n",
    "    (\"G3D_0008_3\", \"G3D_0010_16\", \"G3D_Control_15000\"),\n",
    "    (\"G3D_0008_4\", \"G3D_0010_21\", \"G3D_Control_20000\"),\n",
    "    (\"G3D_0013_1\", \"G3D_0011_1\", \"G3D_Park_1\"),\n",
    "    (\"G3D_0013_11\", \"G3D_0011_2\", \"G3D_Park_1000\"),\n",
    "    (\"G3D_0013_21\", \"G3D_0011_3\", \"G3D_Park_2000\"),\n",
    "    (\"G3D_0009_1\", \"G3D_0011_6\", \"G3D_Park_5000\"),\n",
    "    (\"G3D_0009_2\", \"G3D_0011_11\", \"G3D_Park_10000\"),\n",
    "    (\"G3D_0009_3\", \"G3D_0011_16\", \"G3D_Park_15000\"),\n",
    "    (\"G3D_0009_4\", \"G3D_0011_21\", \"G3D_Park_20000\"),\n",
    "]:\n",
    "    column_ids = np.r_[df.loc[df[\"Exp ID\"] == exp_id_1].shape[0] * [new_id]].reshape([-1, 1])\n",
    "    columns_fields = df.loc[df[\"Exp ID\"] == exp_id_1].values[:, 1:4]\n",
    "    data1 = df.loc[df[\"Exp ID\"] == exp_id_1].values[:, 4:]\n",
    "    data2 = df.loc[df[\"Exp ID\"] == exp_id_2].values[:, 4:]\n",
    "    average = (data1 + data2) / 2\n",
    "    concat_data = pd.DataFrame(np.concatenate([column_ids, columns_fields, average], axis=1), columns = df.columns)\n",
    "    df = pd.concat([df, concat_data], axis=0)\n",
    "\n",
    "df.to_excel(os.path.join(save_path_2_excels, \"classing_metrics.xlsx\"), index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2982b21",
   "metadata": {},
   "source": [
    "### Grouping Qualitative Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d3fd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_table = []\n",
    "data_columns = [\n",
    "    \"Exp ID\", \"Element\", \"G1\", \"G2\", \"Homo\", \"Class\", \n",
    "    \"Chi2 G1 -> G2\", \"Delta Chi2 G1 -> G2\", \"Chi2 G2 -> G1\", \"Delta Chi2 G2 -> G1\"\n",
    "]\n",
    "\n",
    "for t in [\"encoder\", \"contextual\", \"adversarial\"]:\n",
    "    for g1, g2 in [\n",
    "        (\"train\", \"val\"), (\"train\", \"test\"), (\"val\", \"test\"),\n",
    "    ]:\n",
    "        for exp_id in exps:\n",
    "            if \"train_{}_normal\".format(t) in exps[exp_id].keys():\n",
    "                c = \"normal\"\n",
    "            else:\n",
    "                c = \"abnormal\"\n",
    "            data1 = exps[exp_id][\"{}_{}_{}\".format(g1, t, c)]\n",
    "            data2 = exps[exp_id][\"{}_{}_{}\".format(g2, t, c)]\n",
    "            if data1.shape[0] > data2.shape[0]:\n",
    "                sub_data1 = np.r_[sorted(data1)]\n",
    "                sub_data2 = np.r_[sorted(repeat_vector_to_size(data2, data1.shape[0], seed))]\n",
    "            elif data1.shape[0] < data2.shape[0]:\n",
    "                sub_data1 = np.r_[sorted(repeat_vector_to_size(data1, data2.shape[0], seed))]\n",
    "                sub_data2 = np.r_[sorted(data2)]\n",
    "            else:\n",
    "                sub_data1 = np.r_[sorted(data1)]\n",
    "                sub_data2 = np.r_[sorted(data2)]\n",
    "            chi_test_1 = chiSquare_test(sub_data1, sub_data2)\n",
    "            chi_test_2 = chiSquare_test(sub_data2, sub_data1)\n",
    "            data1 = np.r_[sorted(data1)]\n",
    "            data2 = np.r_[sorted(data2)]\n",
    "            homo_level = [int(brownForsythe_test(data1, data2)), int(levene_test(data1, data2))]\n",
    "            class_level = [int(chi_test_1[0]), int(chi_test_2[0])]\n",
    "            data_table.append([\n",
    "                exp_id,\n",
    "                t,\n",
    "                g1,\n",
    "                g2,\n",
    "                np.mean(homo_level),\n",
    "                np.mean(class_level),\n",
    "                int(chi_test_1[0]), \n",
    "                round(chi_test_1[1], 5),\n",
    "                int(chi_test_2[0]), \n",
    "                round(chi_test_2[1], 5),\n",
    "            ])\n",
    "df = pd.DataFrame(data_table, columns=data_columns)\n",
    "\n",
    "for exp_id_1, exp_id_2, new_id in [\n",
    "    (\"G3D_0012_1\", \"G3D_0010_1\", \"G3D_Control_1\"),\n",
    "    (\"G3D_0012_11\", \"G3D_0010_2\", \"G3D_Control_1000\"),\n",
    "    (\"G3D_0012_21\", \"G3D_0010_3\", \"G3D_Control_2000\"),\n",
    "    (\"G3D_0008_1\", \"G3D_0010_6\", \"G3D_Control_5000\"),\n",
    "    (\"G3D_0008_2\", \"G3D_0010_11\", \"G3D_Control_10000\"),\n",
    "    (\"G3D_0008_3\", \"G3D_0010_16\", \"G3D_Control_15000\"),\n",
    "    (\"G3D_0008_4\", \"G3D_0010_21\", \"G3D_Control_20000\"),\n",
    "    (\"G3D_0013_1\", \"G3D_0011_1\", \"G3D_Park_1\"),\n",
    "    (\"G3D_0013_11\", \"G3D_0011_2\", \"G3D_Park_1000\"),\n",
    "    (\"G3D_0013_21\", \"G3D_0011_3\", \"G3D_Park_2000\"),\n",
    "    (\"G3D_0009_1\", \"G3D_0011_6\", \"G3D_Park_5000\"),\n",
    "    (\"G3D_0009_2\", \"G3D_0011_11\", \"G3D_Park_10000\"),\n",
    "    (\"G3D_0009_3\", \"G3D_0011_16\", \"G3D_Park_15000\"),\n",
    "    (\"G3D_0009_4\", \"G3D_0011_21\", \"G3D_Park_20000\"),\n",
    "]:\n",
    "    column_ids = np.r_[df.loc[df[\"Exp ID\"] == exp_id_1].shape[0] * [new_id]].reshape([-1, 1])\n",
    "    columns_fields = df.loc[df[\"Exp ID\"] == exp_id_1].values[:, 1:4]\n",
    "    data1 = df.loc[df[\"Exp ID\"] == exp_id_1].values[:, 4:]\n",
    "    data2 = df.loc[df[\"Exp ID\"] == exp_id_2].values[:, 4:]\n",
    "    average = (data1 + data2) / 2\n",
    "    concat_data = pd.DataFrame(np.concatenate([column_ids, columns_fields, average], axis=1), columns = df.columns)\n",
    "    df = pd.concat([df, concat_data], axis=0)\n",
    "\n",
    "df.to_excel(os.path.join(save_path_2_excels, \"grouping_metrics.xlsx\"), index=False)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
