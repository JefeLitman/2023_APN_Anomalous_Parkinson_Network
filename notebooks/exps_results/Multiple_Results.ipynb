{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "endangered-assets",
   "metadata": {},
   "source": [
    "# Notebook for Multiple Experiment Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protected-meter",
   "metadata": {},
   "source": [
    "### Libraries import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ignored-motorcycle",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-22 15:53:28.312508: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "sys.path.append(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "204151ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.metrics import tpr_fpr_curve, all_metrics_curve\n",
    "from utils.common import format_index, repeat_vector_to_size\n",
    "from utils.metrics import homocedasticity_level, shapeness_level\n",
    "from utils.metrics import accuracy, precision, recall, specificity, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eec12b3",
   "metadata": {},
   "source": [
    "### Experiments selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5551c490",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'G3D_0012_1': {'path': '../../results/Ganomaly_3D/0012_train_healthy/0001_Ganomaly3D-64x64x64x1'},\n",
       " 'G3D_0012_2': {'path': '../../results/Ganomaly_3D/0012_train_healthy/0002_Ganomaly3D-64x64x64x1'},\n",
       " 'G3D_0012_3': {'path': '../../results/Ganomaly_3D/0012_train_healthy/0003_Ganomaly3D-64x64x64x1'},\n",
       " 'G3D_0012_4': {'path': '../../results/Ganomaly_3D/0012_train_healthy/0004_Ganomaly3D-64x64x64x1'},\n",
       " 'G3D_0012_5': {'path': '../../results/Ganomaly_3D/0012_train_healthy/0005_Ganomaly3D-64x64x64x1'},\n",
       " 'G3D_0012_6': {'path': '../../results/Ganomaly_3D/0012_train_healthy/0006_Ganomaly3D-64x64x64x1'},\n",
       " 'G3D_0012_7': {'path': '../../results/Ganomaly_3D/0012_train_healthy/0007_Ganomaly3D-64x64x64x1'},\n",
       " 'G3D_0012_8': {'path': '../../results/Ganomaly_3D/0012_train_healthy/0008_Ganomaly3D-64x64x64x1'},\n",
       " 'G3D_0012_9': {'path': '../../results/Ganomaly_3D/0012_train_healthy/0009_Ganomaly3D-64x64x64x1'},\n",
       " 'G3D_0012_10': {'path': '../../results/Ganomaly_3D/0012_train_healthy/0010_Ganomaly3D-64x64x64x1'},\n",
       " 'G3D_0012_11': {'path': '../../results/Ganomaly_3D/0012_train_healthy/0011_Ganomaly3D-64x64x64x1'},\n",
       " 'G3D_0012_12': {'path': '../../results/Ganomaly_3D/0012_train_healthy/0012_Ganomaly3D-64x64x64x1'},\n",
       " 'G3D_0012_13': {'path': '../../results/Ganomaly_3D/0012_train_healthy/0013_Ganomaly3D-64x64x64x1'},\n",
       " 'G3D_0012_14': {'path': '../../results/Ganomaly_3D/0012_train_healthy/0014_Ganomaly3D-64x64x64x1'},\n",
       " 'G3D_0012_15': {'path': '../../results/Ganomaly_3D/0012_train_healthy/0015_Ganomaly3D-64x64x64x1'},\n",
       " 'G3D_0012_16': {'path': '../../results/Ganomaly_3D/0012_train_healthy/0016_Ganomaly3D-64x64x64x1'},\n",
       " 'G3D_0012_17': {'path': '../../results/Ganomaly_3D/0012_train_healthy/0017_Ganomaly3D-64x64x64x1'},\n",
       " 'G3D_0012_18': {'path': '../../results/Ganomaly_3D/0012_train_healthy/0018_Ganomaly3D-64x64x64x1'},\n",
       " 'G3D_0012_19': {'path': '../../results/Ganomaly_3D/0012_train_healthy/0019_Ganomaly3D-64x64x64x1'},\n",
       " 'G3D_0012_20': {'path': '../../results/Ganomaly_3D/0012_train_healthy/0020_Ganomaly3D-64x64x64x1'},\n",
       " 'G3D_0012_21': {'path': '../../results/Ganomaly_3D/0012_train_healthy/0021_Ganomaly3D-64x64x64x1'},\n",
       " 'G3D_0010_1': {'path': '../../results/Ganomaly_3D/0010_train_healthy/0001_Ganomaly3D-64x64x64x1'},\n",
       " 'G3D_0010_2': {'path': '../../results/Ganomaly_3D/0010_train_healthy/0002_Ganomaly3D-64x64x64x1'},\n",
       " 'G3D_0010_3': {'path': '../../results/Ganomaly_3D/0010_train_healthy/0003_Ganomaly3D-64x64x64x1'},\n",
       " 'G3D_0010_4': {'path': '../../results/Ganomaly_3D/0010_train_healthy/0004_Ganomaly3D-64x64x64x1'},\n",
       " 'G3D_0010_5': {'path': '../../results/Ganomaly_3D/0010_train_healthy/0005_Ganomaly3D-64x64x64x1'},\n",
       " 'G3D_0010_6': {'path': '../../results/Ganomaly_3D/0010_train_healthy/0006_Ganomaly3D-64x64x64x1'},\n",
       " 'G3D_0010_7': {'path': '../../results/Ganomaly_3D/0010_train_healthy/0007_Ganomaly3D-64x64x64x1'},\n",
       " 'G3D_0010_8': {'path': '../../results/Ganomaly_3D/0010_train_healthy/0008_Ganomaly3D-64x64x64x1'},\n",
       " 'G3D_0010_9': {'path': '../../results/Ganomaly_3D/0010_train_healthy/0009_Ganomaly3D-64x64x64x1'},\n",
       " 'G3D_0010_10': {'path': '../../results/Ganomaly_3D/0010_train_healthy/0010_Ganomaly3D-64x64x64x1'},\n",
       " 'G3D_0010_11': {'path': '../../results/Ganomaly_3D/0010_train_healthy/0011_Ganomaly3D-64x64x64x1'},\n",
       " 'G3D_0010_12': {'path': '../../results/Ganomaly_3D/0010_train_healthy/0012_Ganomaly3D-64x64x64x1'},\n",
       " 'G3D_0010_13': {'path': '../../results/Ganomaly_3D/0010_train_healthy/0013_Ganomaly3D-64x64x64x1'},\n",
       " 'G3D_0010_14': {'path': '../../results/Ganomaly_3D/0010_train_healthy/0014_Ganomaly3D-64x64x64x1'},\n",
       " 'G3D_0010_15': {'path': '../../results/Ganomaly_3D/0010_train_healthy/0015_Ganomaly3D-64x64x64x1'},\n",
       " 'G3D_0010_16': {'path': '../../results/Ganomaly_3D/0010_train_healthy/0016_Ganomaly3D-64x64x64x1'},\n",
       " 'G3D_0010_17': {'path': '../../results/Ganomaly_3D/0010_train_healthy/0017_Ganomaly3D-64x64x64x1'},\n",
       " 'G3D_0010_18': {'path': '../../results/Ganomaly_3D/0010_train_healthy/0018_Ganomaly3D-64x64x64x1'},\n",
       " 'G3D_0010_19': {'path': '../../results/Ganomaly_3D/0010_train_healthy/0019_Ganomaly3D-64x64x64x1'},\n",
       " 'G3D_0010_20': {'path': '../../results/Ganomaly_3D/0010_train_healthy/0020_Ganomaly3D-64x64x64x1'},\n",
       " 'G3D_0010_21': {'path': '../../results/Ganomaly_3D/0010_train_healthy/0021_Ganomaly3D-64x64x64x1'},\n",
       " 'G3D_0008_1': {'path': '../../results/Ganomaly_3D/0008_epochs_of_6/0001_Ganomaly3D-64x64x64x1 (5000 epochs)'},\n",
       " 'G3D_0008_2': {'path': '../../results/Ganomaly_3D/0008_epochs_of_6/0002_Ganomaly3D-64x64x64x1 (10000 epochs)'},\n",
       " 'G3D_0008_3': {'path': '../../results/Ganomaly_3D/0008_epochs_of_6/0003_Ganomaly3D-64x64x64x1 (15000 epochs)'},\n",
       " 'G3D_0008_4': {'path': '../../results/Ganomaly_3D/0008_epochs_of_6/0004_Ganomaly3D-64x64x64x1 (20000 epochs)'},\n",
       " 'G3D_0013_1': {'path': '../../results/Ganomaly_3D/0013_train_parkinson/0001_Ganomaly3D-64x64x64x1'},\n",
       " 'G3D_0013_2': {'path': '../../results/Ganomaly_3D/0013_train_parkinson/0002_Ganomaly3D-64x64x64x1'},\n",
       " 'G3D_0013_3': {'path': '../../results/Ganomaly_3D/0013_train_parkinson/0003_Ganomaly3D-64x64x64x1'},\n",
       " 'G3D_0013_4': {'path': '../../results/Ganomaly_3D/0013_train_parkinson/0004_Ganomaly3D-64x64x64x1'},\n",
       " 'G3D_0013_5': {'path': '../../results/Ganomaly_3D/0013_train_parkinson/0005_Ganomaly3D-64x64x64x1'},\n",
       " 'G3D_0013_6': {'path': '../../results/Ganomaly_3D/0013_train_parkinson/0006_Ganomaly3D-64x64x64x1'},\n",
       " 'G3D_0013_7': {'path': '../../results/Ganomaly_3D/0013_train_parkinson/0007_Ganomaly3D-64x64x64x1'},\n",
       " 'G3D_0013_8': {'path': '../../results/Ganomaly_3D/0013_train_parkinson/0008_Ganomaly3D-64x64x64x1'},\n",
       " 'G3D_0013_9': {'path': '../../results/Ganomaly_3D/0013_train_parkinson/0009_Ganomaly3D-64x64x64x1'},\n",
       " 'G3D_0013_10': {'path': '../../results/Ganomaly_3D/0013_train_parkinson/0010_Ganomaly3D-64x64x64x1'},\n",
       " 'G3D_0013_11': {'path': '../../results/Ganomaly_3D/0013_train_parkinson/0011_Ganomaly3D-64x64x64x1'},\n",
       " 'G3D_0013_12': {'path': '../../results/Ganomaly_3D/0013_train_parkinson/0012_Ganomaly3D-64x64x64x1'},\n",
       " 'G3D_0013_13': {'path': '../../results/Ganomaly_3D/0013_train_parkinson/0013_Ganomaly3D-64x64x64x1'},\n",
       " 'G3D_0013_14': {'path': '../../results/Ganomaly_3D/0013_train_parkinson/0014_Ganomaly3D-64x64x64x1'},\n",
       " 'G3D_0013_15': {'path': '../../results/Ganomaly_3D/0013_train_parkinson/0015_Ganomaly3D-64x64x64x1'},\n",
       " 'G3D_0013_16': {'path': '../../results/Ganomaly_3D/0013_train_parkinson/0016_Ganomaly3D-64x64x64x1'},\n",
       " 'G3D_0013_17': {'path': '../../results/Ganomaly_3D/0013_train_parkinson/0017_Ganomaly3D-64x64x64x1'},\n",
       " 'G3D_0013_18': {'path': '../../results/Ganomaly_3D/0013_train_parkinson/0018_Ganomaly3D-64x64x64x1'},\n",
       " 'G3D_0013_19': {'path': '../../results/Ganomaly_3D/0013_train_parkinson/0019_Ganomaly3D-64x64x64x1'},\n",
       " 'G3D_0013_20': {'path': '../../results/Ganomaly_3D/0013_train_parkinson/0020_Ganomaly3D-64x64x64x1'},\n",
       " 'G3D_0013_21': {'path': '../../results/Ganomaly_3D/0013_train_parkinson/0021_Ganomaly3D-64x64x64x1'},\n",
       " 'G3D_0011_1': {'path': '../../results/Ganomaly_3D/0011_train_parkinson/0001_Ganomaly3D-64x64x64x1'},\n",
       " 'G3D_0011_2': {'path': '../../results/Ganomaly_3D/0011_train_parkinson/0002_Ganomaly3D-64x64x64x1'},\n",
       " 'G3D_0011_3': {'path': '../../results/Ganomaly_3D/0011_train_parkinson/0003_Ganomaly3D-64x64x64x1'},\n",
       " 'G3D_0011_4': {'path': '../../results/Ganomaly_3D/0011_train_parkinson/0004_Ganomaly3D-64x64x64x1'},\n",
       " 'G3D_0011_5': {'path': '../../results/Ganomaly_3D/0011_train_parkinson/0005_Ganomaly3D-64x64x64x1'},\n",
       " 'G3D_0011_6': {'path': '../../results/Ganomaly_3D/0011_train_parkinson/0006_Ganomaly3D-64x64x64x1'},\n",
       " 'G3D_0011_7': {'path': '../../results/Ganomaly_3D/0011_train_parkinson/0007_Ganomaly3D-64x64x64x1'},\n",
       " 'G3D_0011_8': {'path': '../../results/Ganomaly_3D/0011_train_parkinson/0008_Ganomaly3D-64x64x64x1'},\n",
       " 'G3D_0011_9': {'path': '../../results/Ganomaly_3D/0011_train_parkinson/0009_Ganomaly3D-64x64x64x1'},\n",
       " 'G3D_0011_10': {'path': '../../results/Ganomaly_3D/0011_train_parkinson/0010_Ganomaly3D-64x64x64x1'},\n",
       " 'G3D_0011_11': {'path': '../../results/Ganomaly_3D/0011_train_parkinson/0011_Ganomaly3D-64x64x64x1'},\n",
       " 'G3D_0011_12': {'path': '../../results/Ganomaly_3D/0011_train_parkinson/0012_Ganomaly3D-64x64x64x1'},\n",
       " 'G3D_0011_13': {'path': '../../results/Ganomaly_3D/0011_train_parkinson/0013_Ganomaly3D-64x64x64x1'},\n",
       " 'G3D_0011_14': {'path': '../../results/Ganomaly_3D/0011_train_parkinson/0014_Ganomaly3D-64x64x64x1'},\n",
       " 'G3D_0011_15': {'path': '../../results/Ganomaly_3D/0011_train_parkinson/0015_Ganomaly3D-64x64x64x1'},\n",
       " 'G3D_0011_16': {'path': '../../results/Ganomaly_3D/0011_train_parkinson/0016_Ganomaly3D-64x64x64x1'},\n",
       " 'G3D_0011_17': {'path': '../../results/Ganomaly_3D/0011_train_parkinson/0017_Ganomaly3D-64x64x64x1'},\n",
       " 'G3D_0011_18': {'path': '../../results/Ganomaly_3D/0011_train_parkinson/0018_Ganomaly3D-64x64x64x1'},\n",
       " 'G3D_0011_19': {'path': '../../results/Ganomaly_3D/0011_train_parkinson/0019_Ganomaly3D-64x64x64x1'},\n",
       " 'G3D_0011_20': {'path': '../../results/Ganomaly_3D/0011_train_parkinson/0020_Ganomaly3D-64x64x64x1'},\n",
       " 'G3D_0011_21': {'path': '../../results/Ganomaly_3D/0011_train_parkinson/0021_Ganomaly3D-64x64x64x1'},\n",
       " 'G3D_0009_1': {'path': '../../results/Ganomaly_3D/0009_epochs_of_7/0001_Ganomaly3D-64x64x64x1 (5000 epochs)'},\n",
       " 'G3D_0009_2': {'path': '../../results/Ganomaly_3D/0009_epochs_of_7/0002_Ganomaly3D-64x64x64x1 (10000 epochs)'},\n",
       " 'G3D_0009_3': {'path': '../../results/Ganomaly_3D/0009_epochs_of_7/0003_Ganomaly3D-64x64x64x1 (15000 epochs)'},\n",
       " 'G3D_0009_4': {'path': '../../results/Ganomaly_3D/0009_epochs_of_7/0004_Ganomaly3D-64x64x64x1 (20000 epochs)'}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "major_exps_ids = [format_index(i) for i in [12, 10, 8, 13, 11, 9]]\n",
    "minor_exps_ids = [format_index(i) for i in range(1, 22)]\n",
    "# major_exps_ids = [format_index(i) for i in [12, 14]]\n",
    "# minor_4_major_ids = {\"0012\": [\"0004\", \"0013\"], \"0014\":[\"0002\",\"0010\"]}\n",
    "save_path_2_excels = \"../../results/Results_Reports/\"\n",
    "\n",
    "exps = {}\n",
    "seed = 8128\n",
    "root_path = \"../../results/Ganomaly_3D/\"\n",
    "\n",
    "for i in major_exps_ids:\n",
    "    for exp_id in sorted(os.listdir(root_path)):\n",
    "        if i in exp_id:\n",
    "            sub_path = os.path.join(root_path, exp_id)\n",
    "            for j in minor_exps_ids:#minor_4_major_ids[i]:\n",
    "                for subexp_id in sorted(os.listdir(sub_path)):\n",
    "                    if j in subexp_id:\n",
    "                        final_id = \"G3D_{}_{}\".format(i, int(j))\n",
    "                        exp_path = os.path.join(sub_path, subexp_id)\n",
    "                        exps[final_id] = {\n",
    "                            \"path\": exp_path\n",
    "                        }\n",
    "\n",
    "exps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01ba969",
   "metadata": {},
   "source": [
    "### Errors loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf295a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "for exp_id in exps:\n",
    "    errors_path = os.path.join(exps[exp_id][\"path\"], \"outputs/errors\")\n",
    "    vectors_path = os.path.join(exps[exp_id][\"path\"], \"outputs/latent_vectors/input_generator\")\n",
    "\n",
    "    # Initializing dict for losses of different elements in network\n",
    "    for t in [\"encoder\", \"contextual\", \"adversarial\"]:\n",
    "        data = \"{}_losses\".format(t)\n",
    "        exps[exp_id][data] = {}\n",
    "        for c in [\"normal\", \"abnormal\"]:\n",
    "            exps[exp_id][data][c] = {}\n",
    "            for m in [\"train\", \"val\", \"test\"]:\n",
    "                exps[exp_id][data][c][m] = {}\n",
    "\n",
    "    for t in [\"encoder\", \"contextual\", \"adversarial\"]:\n",
    "        for m in [\"train\", \"val\", \"test\"]:\n",
    "            if m == \"train\":\n",
    "                if os.path.isfile(os.path.join(errors_path, t, m, \"normal.npy\")):\n",
    "                    classes = [\"normal\"] \n",
    "                else:\n",
    "                    classes = [\"abnormal\"]\n",
    "            else:\n",
    "                classes = [\"normal\", \"abnormal\"]\n",
    "\n",
    "            for c in classes:\n",
    "                error_file = os.path.join(errors_path, t, m, c + \".npy\")\n",
    "                if os.path.isfile(error_file):\n",
    "                    data = \"{}_losses\".format(t)\n",
    "                    errors = np.load(error_file)\n",
    "                    patients_ids_positions = [\n",
    "                        int(i.split(\"_\")[1].split(\"-\")[1]) for i in sorted(\n",
    "                            os.listdir(os.path.join(vectors_path, m, c))\n",
    "                        )\n",
    "                    ]\n",
    "                    assert len(errors) == len(patients_ids_positions)\n",
    "                    for p_id in np.unique(patients_ids_positions):\n",
    "                        exps[exp_id][data][c][m][p_id] = []\n",
    "\n",
    "                    for i, p_id in enumerate(patients_ids_positions):\n",
    "                        exps[exp_id][data][c][m][p_id].append(errors[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9d53c3",
   "metadata": {},
   "source": [
    "### Quantitative metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4272ffa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Exp ID</th>\n",
       "      <th>Major ID</th>\n",
       "      <th>Minor ID</th>\n",
       "      <th>Group</th>\n",
       "      <th>Partition</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Threshold</th>\n",
       "      <th>Acc</th>\n",
       "      <th>Pre</th>\n",
       "      <th>Rec</th>\n",
       "      <th>Spe</th>\n",
       "      <th>F1</th>\n",
       "      <th>Homo</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>G3D_0012_1</td>\n",
       "      <td>0012</td>\n",
       "      <td>1</td>\n",
       "      <td>encoder</td>\n",
       "      <td>val</td>\n",
       "      <td>0.667</td>\n",
       "      <td>5.963</td>\n",
       "      <td>0.833</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>G3D_0012_1</td>\n",
       "      <td>0012</td>\n",
       "      <td>1</td>\n",
       "      <td>encoder</td>\n",
       "      <td>test</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>5.963</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>G3D_0012_2</td>\n",
       "      <td>0012</td>\n",
       "      <td>2</td>\n",
       "      <td>encoder</td>\n",
       "      <td>val</td>\n",
       "      <td>0.389</td>\n",
       "      <td>0.764</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.533</td>\n",
       "      <td>0.588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>G3D_0012_2</td>\n",
       "      <td>0012</td>\n",
       "      <td>2</td>\n",
       "      <td>encoder</td>\n",
       "      <td>test</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.764</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.533</td>\n",
       "      <td>0.588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>G3D_0012_3</td>\n",
       "      <td>0012</td>\n",
       "      <td>3</td>\n",
       "      <td>encoder</td>\n",
       "      <td>val</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>G3D_Park_20000</td>\n",
       "      <td>0009</td>\n",
       "      <td>4</td>\n",
       "      <td>encoder</td>\n",
       "      <td>test</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.151</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.582</td>\n",
       "      <td>0.591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>G3D_Park_20000</td>\n",
       "      <td>0009</td>\n",
       "      <td>4</td>\n",
       "      <td>contextual</td>\n",
       "      <td>val</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.833</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.857</td>\n",
       "      <td>0.545</td>\n",
       "      <td>0.6185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>G3D_Park_20000</td>\n",
       "      <td>0009</td>\n",
       "      <td>4</td>\n",
       "      <td>contextual</td>\n",
       "      <td>test</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.545</td>\n",
       "      <td>0.6185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>G3D_Park_20000</td>\n",
       "      <td>0009</td>\n",
       "      <td>4</td>\n",
       "      <td>adversarial</td>\n",
       "      <td>val</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.3815</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.571</td>\n",
       "      <td>0.5295</td>\n",
       "      <td>0.591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>G3D_Park_20000</td>\n",
       "      <td>0009</td>\n",
       "      <td>4</td>\n",
       "      <td>adversarial</td>\n",
       "      <td>test</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.3815</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.5295</td>\n",
       "      <td>0.591</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>636 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Exp ID Major ID Minor ID        Group Partition    AUC Threshold  \\\n",
       "0       G3D_0012_1     0012        1      encoder       val  0.667     5.963   \n",
       "1       G3D_0012_1     0012        1      encoder      test   -0.0     5.963   \n",
       "2       G3D_0012_2     0012        2      encoder       val  0.389     0.764   \n",
       "3       G3D_0012_2     0012        2      encoder      test   -0.0     0.764   \n",
       "4       G3D_0012_3     0012        3      encoder       val  0.222     0.118   \n",
       "..             ...      ...      ...          ...       ...    ...       ...   \n",
       "1   G3D_Park_20000     0009        4      encoder      test  0.125     0.151   \n",
       "2   G3D_Park_20000     0009        4   contextual       val  0.333     0.133   \n",
       "3   G3D_Park_20000     0009        4   contextual      test  0.125     0.133   \n",
       "4   G3D_Park_20000     0009        4  adversarial       val  0.222    0.3815   \n",
       "5   G3D_Park_20000     0009        4  adversarial      test  0.375    0.3815   \n",
       "\n",
       "      Acc    Pre    Rec    Spe     F1    Homo   Class  \n",
       "0   0.833    1.0  0.667    1.0    0.8   0.538   0.417  \n",
       "1    0.75  0.667    1.0    0.5    0.8   0.538   0.417  \n",
       "2   0.333    0.4  0.667    0.0    0.5   0.533   0.588  \n",
       "3    0.75  0.667    1.0    0.5    0.8   0.533   0.588  \n",
       "4   0.667    0.6    1.0  0.333   0.75    0.55   0.636  \n",
       "..    ...    ...    ...    ...    ...     ...     ...  \n",
       "1     0.5    0.5    1.0    0.0  0.667   0.582   0.591  \n",
       "2   0.833   0.75    1.0  0.667  0.857   0.545  0.6185  \n",
       "3     0.5    0.5    1.0    0.0  0.667   0.545  0.6185  \n",
       "4     0.5    0.5  0.667  0.333  0.571  0.5295   0.591  \n",
       "5     0.5    0.5    1.0    0.0  0.667  0.5295   0.591  \n",
       "\n",
       "[636 rows x 14 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_table = []\n",
    "lambda_value = 5\n",
    "data_columns = [\"Exp ID\", \"Major ID\", \"Minor ID\", \"Group\", \"Partition\", \"AUC\", \"Threshold\", \"Acc\", \"Pre\", \"Rec\", \"Spe\", \"F1\", \"Homo\", \"Class\"]\n",
    "\n",
    "for t in [\"encoder\", \"contextual\", \"adversarial\"]:\n",
    "    for exp_id in exps:\n",
    "        for part in [\"val\", \"test\"]:\n",
    "            data = \"{}_losses\".format(t)\n",
    "            if len(exps[exp_id][data][\"abnormal\"][\"train\"]) != 0:\n",
    "                errors = [\"abnormal\", \"normal\"]\n",
    "            else:\n",
    "                errors = [\"normal\", \"abnormal\"]\n",
    "            y_true = []\n",
    "            y_pred = []\n",
    "            for ci, c in enumerate(errors):\n",
    "                samples = exps[exp_id][data][c][part]\n",
    "                y_pred += [\n",
    "                    np.max(samples[i]) for i in samples\n",
    "#                     np.mean(samples[i]) + lambda_value*np.std(samples[i]) for i in samples\n",
    "                ]\n",
    "                y_true += [ci]*len(samples)\n",
    "            y_true = np.r_[y_true]\n",
    "            y_pred = np.r_[y_pred]\n",
    "            tpr, fpr, _ = tpr_fpr_curve(y_true, y_pred)\n",
    "\n",
    "            if part == \"val\":\n",
    "                accs, pres, recs, spes, f1s, thresholds = all_metrics_curve(y_true, y_pred)\n",
    "                table_metrics = np.concatenate([\n",
    "                    accs.reshape([-1,1]), \n",
    "    #                 pres.reshape([-1,1]), \n",
    "                    recs.reshape([-1,1]), \n",
    "    #                 spes.reshape([-1,1]), \n",
    "    #                 f1s.reshape([-1,1])\n",
    "                ], axis=1)\n",
    "                threshold = thresholds[np.argmax(np.mean(table_metrics, axis=1))]\n",
    "                # deltas = np.abs(tpr - fpr)\n",
    "                # threshold = thresholds[np.argmin(deltas[deltas != 0])]\n",
    "\n",
    "            #threshold = 1.174\n",
    "            y_pred = (y_pred > threshold).astype(np.int64)\n",
    "\n",
    "            TP = tf.keras.metrics.TruePositives()\n",
    "            TN = tf.keras.metrics.TrueNegatives()\n",
    "            FP = tf.keras.metrics.FalsePositives()\n",
    "            FN = tf.keras.metrics.FalseNegatives()\n",
    "\n",
    "            TP.update_state(y_true, y_pred)\n",
    "            TN.update_state(y_true, y_pred)\n",
    "            FP.update_state(y_true, y_pred)\n",
    "            FN.update_state(y_true, y_pred)\n",
    "\n",
    "            classes_data = []\n",
    "            for c in [\"normal\", \"abnormal\"]:\n",
    "                parts = []\n",
    "                for g in [\"train\", \"val\", \"test\"]:\n",
    "                    samples = exps[exp_id][data][c][g]\n",
    "                    for p_id in samples:\n",
    "                        parts.append(np.r_[samples[p_id]])\n",
    "                classes_data.append(parts)\n",
    "\n",
    "\n",
    "            data_table.append([\n",
    "                exp_id,\n",
    "                exp_id.split(\"_\")[1],\n",
    "                exp_id.split(\"_\")[2],\n",
    "                t,\n",
    "                part,\n",
    "                round(auc(fpr, tpr), 3),\n",
    "                round(threshold, 3),\n",
    "                round(accuracy(TP.result().numpy(), TN.result().numpy(), FP.result().numpy(), FN.result().numpy()), 3),\n",
    "                round(precision(TP.result().numpy(), FP.result().numpy()), 3),\n",
    "                round(recall(TP.result().numpy(), FN.result().numpy()), 3),\n",
    "                round(specificity(TN.result().numpy(), FP.result().numpy()), 3),\n",
    "                round(f1_score(TP.result().numpy(), FP.result().numpy(), FN.result().numpy()), 3),\n",
    "                round(homocedasticity_level(*classes_data), 3),\n",
    "                round(shapeness_level(*classes_data, seed=seed), 3)\n",
    "            ])\n",
    "df = pd.DataFrame(data_table, columns=data_columns)\n",
    "\n",
    "for exp_id_1, exp_id_2, new_id in [\n",
    "    (\"G3D_0012_1\", \"G3D_0010_1\", \"G3D_Control_1\"),\n",
    "    (\"G3D_0012_11\", \"G3D_0010_2\", \"G3D_Control_1000\"),\n",
    "    (\"G3D_0012_21\", \"G3D_0010_3\", \"G3D_Control_2000\"),\n",
    "    (\"G3D_0008_1\", \"G3D_0010_6\", \"G3D_Control_5000\"),\n",
    "    (\"G3D_0008_2\", \"G3D_0010_11\", \"G3D_Control_10000\"),\n",
    "    (\"G3D_0008_3\", \"G3D_0010_16\", \"G3D_Control_15000\"),\n",
    "    (\"G3D_0008_4\", \"G3D_0010_21\", \"G3D_Control_20000\"),\n",
    "    (\"G3D_0013_1\", \"G3D_0011_1\", \"G3D_Park_1\"),\n",
    "    (\"G3D_0013_11\", \"G3D_0011_2\", \"G3D_Park_1000\"),\n",
    "    (\"G3D_0013_21\", \"G3D_0011_3\", \"G3D_Park_2000\"),\n",
    "    (\"G3D_0009_1\", \"G3D_0011_6\", \"G3D_Park_5000\"),\n",
    "    (\"G3D_0009_2\", \"G3D_0011_11\", \"G3D_Park_10000\"),\n",
    "    (\"G3D_0009_3\", \"G3D_0011_16\", \"G3D_Park_15000\"),\n",
    "    (\"G3D_0009_4\", \"G3D_0011_21\", \"G3D_Park_20000\"),\n",
    "]:\n",
    "    column_ids = np.r_[df.loc[df[\"Exp ID\"] == exp_id_1].shape[0] * [new_id]].reshape([-1, 1])\n",
    "    columns_fields = df.loc[df[\"Exp ID\"] == exp_id_1].values[:, 1:5]\n",
    "    data1 = df.loc[df[\"Exp ID\"] == exp_id_1].values[:, 5:]\n",
    "    data2 = df.loc[df[\"Exp ID\"] == exp_id_2].values[:, 5:]\n",
    "    average = (data1 + data2) / 2\n",
    "    concat_data = pd.DataFrame(np.concatenate([column_ids, columns_fields, average], axis=1), columns = df.columns)\n",
    "    df = pd.concat([df, concat_data], axis=0)\n",
    "\n",
    "# df.to_excel(os.path.join(save_path_2_excels, \"quantitative_metrics_lambda-{}.xlsx\".format(lambda_value)), index=False)\n",
    "df.to_excel(os.path.join(save_path_2_excels, \"quantitative_metrics_max.xlsx\"), index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36eca6b8",
   "metadata": {},
   "source": [
    "### Classing Qualitative Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffc3f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_table = []\n",
    "data_columns = [\n",
    "    \"Exp ID\", \"Element\", \"Group\", \"vs Group\", \"Homo\", \"Class\", \n",
    "    \"Chi2 N -> A\", \"Delta Chi2 N -> A\", \"Chi2 A -> N\", \"Delta Chi2 A -> N\"\n",
    "]\n",
    "for t in [\"encoder\", \"contextual\", \"adversarial\"]:\n",
    "    for g1 in [\"train\", \"val\", \"test\"]:\n",
    "        for g2 in [\"val\", \"test\"]:\n",
    "            for exp_id in exps:\n",
    "                if \"train_{}_normal\".format(t) in exps[exp_id].keys():\n",
    "                    data1 = exps[exp_id][\"{}_{}_normal\".format(g1, t)]\n",
    "                    data2 = exps[exp_id][\"{}_{}_abnormal\".format(g2, t)]\n",
    "                else:\n",
    "                    data2 = exps[exp_id][\"{}_{}_abnormal\".format(g1, t)]\n",
    "                    data1 = exps[exp_id][\"{}_{}_normal\".format(g2, t)]\n",
    "                if data1.shape[0] > data2.shape[0]:\n",
    "                    sub_data1 = np.r_[sorted(data1)]\n",
    "                    sub_data2 = np.r_[sorted(repeat_vector_to_size(data2, data1.shape[0], seed))]\n",
    "                elif data1.shape[0] < data2.shape[0]:\n",
    "                    sub_data1 = np.r_[sorted(repeat_vector_to_size(data1, data2.shape[0], seed))]\n",
    "                    sub_data2 = np.r_[sorted(data2)]\n",
    "                else:\n",
    "                    sub_data1 = np.r_[sorted(data1)]\n",
    "                    sub_data2 = np.r_[sorted(data2)]\n",
    "                chi_test_1 = chiSquare_test(sub_data1, sub_data2)\n",
    "                chi_test_2 = chiSquare_test(sub_data2, sub_data1)\n",
    "                data1 = np.r_[sorted(data1)]\n",
    "                data2 = np.r_[sorted(data2)]\n",
    "                homo_level = [abs(1 - int(brownForsythe_test(data1, data2))), abs(1 - int(levene_test(data1, data2)))]\n",
    "                class_level = [abs(1 - int(chi_test_1[0])), abs(1 - int(chi_test_2[0]))]\n",
    "                data_table.append([\n",
    "                    exp_id,\n",
    "                    t,\n",
    "                    g1,\n",
    "                    g2, \n",
    "                    np.mean(homo_level),\n",
    "                    np.mean(class_level),\n",
    "                    int(chi_test_1[0]), \n",
    "                    round(chi_test_1[1], 5),\n",
    "                    int(chi_test_2[0]), \n",
    "                    round(chi_test_2[1], 5),\n",
    "                ])\n",
    "df = pd.DataFrame(data_table, columns=data_columns)\n",
    "\n",
    "for exp_id_1, exp_id_2, new_id in [\n",
    "    (\"G3D_0012_1\", \"G3D_0010_1\", \"G3D_Control_1\"),\n",
    "    (\"G3D_0012_11\", \"G3D_0010_2\", \"G3D_Control_1000\"),\n",
    "    (\"G3D_0012_21\", \"G3D_0010_3\", \"G3D_Control_2000\"),\n",
    "    (\"G3D_0008_1\", \"G3D_0010_6\", \"G3D_Control_5000\"),\n",
    "    (\"G3D_0008_2\", \"G3D_0010_11\", \"G3D_Control_10000\"),\n",
    "    (\"G3D_0008_3\", \"G3D_0010_16\", \"G3D_Control_15000\"),\n",
    "    (\"G3D_0008_4\", \"G3D_0010_21\", \"G3D_Control_20000\"),\n",
    "    (\"G3D_0013_1\", \"G3D_0011_1\", \"G3D_Park_1\"),\n",
    "    (\"G3D_0013_11\", \"G3D_0011_2\", \"G3D_Park_1000\"),\n",
    "    (\"G3D_0013_21\", \"G3D_0011_3\", \"G3D_Park_2000\"),\n",
    "    (\"G3D_0009_1\", \"G3D_0011_6\", \"G3D_Park_5000\"),\n",
    "    (\"G3D_0009_2\", \"G3D_0011_11\", \"G3D_Park_10000\"),\n",
    "    (\"G3D_0009_3\", \"G3D_0011_16\", \"G3D_Park_15000\"),\n",
    "    (\"G3D_0009_4\", \"G3D_0011_21\", \"G3D_Park_20000\"),\n",
    "]:\n",
    "    column_ids = np.r_[df.loc[df[\"Exp ID\"] == exp_id_1].shape[0] * [new_id]].reshape([-1, 1])\n",
    "    columns_fields = df.loc[df[\"Exp ID\"] == exp_id_1].values[:, 1:4]\n",
    "    data1 = df.loc[df[\"Exp ID\"] == exp_id_1].values[:, 4:]\n",
    "    data2 = df.loc[df[\"Exp ID\"] == exp_id_2].values[:, 4:]\n",
    "    average = (data1 + data2) / 2\n",
    "    concat_data = pd.DataFrame(np.concatenate([column_ids, columns_fields, average], axis=1), columns = df.columns)\n",
    "    df = pd.concat([df, concat_data], axis=0)\n",
    "\n",
    "df.to_excel(os.path.join(save_path_2_excels, \"classing_metrics.xlsx\"), index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2982b21",
   "metadata": {},
   "source": [
    "### Grouping Qualitative Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d3fd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_table = []\n",
    "data_columns = [\n",
    "    \"Exp ID\", \"Element\", \"G1\", \"G2\", \"Homo\", \"Class\", \n",
    "    \"Chi2 G1 -> G2\", \"Delta Chi2 G1 -> G2\", \"Chi2 G2 -> G1\", \"Delta Chi2 G2 -> G1\"\n",
    "]\n",
    "\n",
    "for t in [\"encoder\", \"contextual\", \"adversarial\"]:\n",
    "    for g1, g2 in [\n",
    "        (\"train\", \"val\"), (\"train\", \"test\"), (\"val\", \"test\"),\n",
    "    ]:\n",
    "        for exp_id in exps:\n",
    "            if \"train_{}_normal\".format(t) in exps[exp_id].keys():\n",
    "                c = \"normal\"\n",
    "            else:\n",
    "                c = \"abnormal\"\n",
    "            data1 = exps[exp_id][\"{}_{}_{}\".format(g1, t, c)]\n",
    "            data2 = exps[exp_id][\"{}_{}_{}\".format(g2, t, c)]\n",
    "            if data1.shape[0] > data2.shape[0]:\n",
    "                sub_data1 = np.r_[sorted(data1)]\n",
    "                sub_data2 = np.r_[sorted(repeat_vector_to_size(data2, data1.shape[0], seed))]\n",
    "            elif data1.shape[0] < data2.shape[0]:\n",
    "                sub_data1 = np.r_[sorted(repeat_vector_to_size(data1, data2.shape[0], seed))]\n",
    "                sub_data2 = np.r_[sorted(data2)]\n",
    "            else:\n",
    "                sub_data1 = np.r_[sorted(data1)]\n",
    "                sub_data2 = np.r_[sorted(data2)]\n",
    "            chi_test_1 = chiSquare_test(sub_data1, sub_data2)\n",
    "            chi_test_2 = chiSquare_test(sub_data2, sub_data1)\n",
    "            data1 = np.r_[sorted(data1)]\n",
    "            data2 = np.r_[sorted(data2)]\n",
    "            homo_level = [int(brownForsythe_test(data1, data2)), int(levene_test(data1, data2))]\n",
    "            class_level = [int(chi_test_1[0]), int(chi_test_2[0])]\n",
    "            data_table.append([\n",
    "                exp_id,\n",
    "                t,\n",
    "                g1,\n",
    "                g2,\n",
    "                np.mean(homo_level),\n",
    "                np.mean(class_level),\n",
    "                int(chi_test_1[0]), \n",
    "                round(chi_test_1[1], 5),\n",
    "                int(chi_test_2[0]), \n",
    "                round(chi_test_2[1], 5),\n",
    "            ])\n",
    "df = pd.DataFrame(data_table, columns=data_columns)\n",
    "\n",
    "for exp_id_1, exp_id_2, new_id in [\n",
    "    (\"G3D_0012_1\", \"G3D_0010_1\", \"G3D_Control_1\"),\n",
    "    (\"G3D_0012_11\", \"G3D_0010_2\", \"G3D_Control_1000\"),\n",
    "    (\"G3D_0012_21\", \"G3D_0010_3\", \"G3D_Control_2000\"),\n",
    "    (\"G3D_0008_1\", \"G3D_0010_6\", \"G3D_Control_5000\"),\n",
    "    (\"G3D_0008_2\", \"G3D_0010_11\", \"G3D_Control_10000\"),\n",
    "    (\"G3D_0008_3\", \"G3D_0010_16\", \"G3D_Control_15000\"),\n",
    "    (\"G3D_0008_4\", \"G3D_0010_21\", \"G3D_Control_20000\"),\n",
    "    (\"G3D_0013_1\", \"G3D_0011_1\", \"G3D_Park_1\"),\n",
    "    (\"G3D_0013_11\", \"G3D_0011_2\", \"G3D_Park_1000\"),\n",
    "    (\"G3D_0013_21\", \"G3D_0011_3\", \"G3D_Park_2000\"),\n",
    "    (\"G3D_0009_1\", \"G3D_0011_6\", \"G3D_Park_5000\"),\n",
    "    (\"G3D_0009_2\", \"G3D_0011_11\", \"G3D_Park_10000\"),\n",
    "    (\"G3D_0009_3\", \"G3D_0011_16\", \"G3D_Park_15000\"),\n",
    "    (\"G3D_0009_4\", \"G3D_0011_21\", \"G3D_Park_20000\"),\n",
    "]:\n",
    "    column_ids = np.r_[df.loc[df[\"Exp ID\"] == exp_id_1].shape[0] * [new_id]].reshape([-1, 1])\n",
    "    columns_fields = df.loc[df[\"Exp ID\"] == exp_id_1].values[:, 1:4]\n",
    "    data1 = df.loc[df[\"Exp ID\"] == exp_id_1].values[:, 4:]\n",
    "    data2 = df.loc[df[\"Exp ID\"] == exp_id_2].values[:, 4:]\n",
    "    average = (data1 + data2) / 2\n",
    "    concat_data = pd.DataFrame(np.concatenate([column_ids, columns_fields, average], axis=1), columns = df.columns)\n",
    "    df = pd.concat([df, concat_data], axis=0)\n",
    "\n",
    "df.to_excel(os.path.join(save_path_2_excels, \"grouping_metrics.xlsx\"), index=False)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
